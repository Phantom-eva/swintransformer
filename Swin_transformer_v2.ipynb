{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Swin_transformer_v2.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyMNrGgtiqL4IkEmS7eL3t3h"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["Import necessary packages."],"metadata":{"id":"79lDxK9moDzi"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"ihicZ8_dnRzu","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1650834639112,"user_tz":240,"elapsed":17908,"user":{"displayName":"Yilin Li","userId":"08057653249073875200"}},"outputId":"4807099b-baae-4b27-d9ca-643a7dc01610"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting timm\n","  Downloading timm-0.5.4-py3-none-any.whl (431 kB)\n","\u001b[?25l\r\u001b[K     |▊                               | 10 kB 11.3 MB/s eta 0:00:01\r\u001b[K     |█▌                              | 20 kB 14.8 MB/s eta 0:00:01\r\u001b[K     |██▎                             | 30 kB 9.8 MB/s eta 0:00:01\r\u001b[K     |███                             | 40 kB 8.9 MB/s eta 0:00:01\r\u001b[K     |███▉                            | 51 kB 7.1 MB/s eta 0:00:01\r\u001b[K     |████▋                           | 61 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 71 kB 8.1 MB/s eta 0:00:01\r\u001b[K     |██████                          | 81 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |██████▉                         | 92 kB 7.0 MB/s eta 0:00:01\r\u001b[K     |███████▋                        | 102 kB 6.1 MB/s eta 0:00:01\r\u001b[K     |████████▍                       | 112 kB 6.1 MB/s eta 0:00:01\r\u001b[K     |█████████▏                      | 122 kB 6.1 MB/s eta 0:00:01\r\u001b[K     |█████████▉                      | 133 kB 6.1 MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 143 kB 6.1 MB/s eta 0:00:01\r\u001b[K     |███████████▍                    | 153 kB 6.1 MB/s eta 0:00:01\r\u001b[K     |████████████▏                   | 163 kB 6.1 MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 174 kB 6.1 MB/s eta 0:00:01\r\u001b[K     |█████████████▊                  | 184 kB 6.1 MB/s eta 0:00:01\r\u001b[K     |██████████████▍                 | 194 kB 6.1 MB/s eta 0:00:01\r\u001b[K     |███████████████▏                | 204 kB 6.1 MB/s eta 0:00:01\r\u001b[K     |████████████████                | 215 kB 6.1 MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 225 kB 6.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████▌              | 235 kB 6.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████▎             | 245 kB 6.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 256 kB 6.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████▊            | 266 kB 6.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████▌           | 276 kB 6.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▎          | 286 kB 6.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 296 kB 6.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▉         | 307 kB 6.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▌        | 317 kB 6.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 327 kB 6.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 337 kB 6.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▉      | 348 kB 6.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▋     | 358 kB 6.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▍    | 368 kB 6.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 378 kB 6.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▉   | 389 kB 6.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▋  | 399 kB 6.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▍ | 409 kB 6.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 419 kB 6.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 430 kB 6.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 431 kB 6.1 MB/s \n","\u001b[?25hRequirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from timm) (0.11.1+cu111)\n","Requirement already satisfied: torch>=1.4 in /usr/local/lib/python3.7/dist-packages (from timm) (1.10.0+cu111)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.4->timm) (4.1.1)\n","Requirement already satisfied: pillow!=8.3.0,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision->timm) (7.1.2)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchvision->timm) (1.21.6)\n","Installing collected packages: timm\n","Successfully installed timm-0.5.4\n","PyTorch Version:  1.10.0+cu111\n","Torchvision Version:  0.11.1+cu111\n","cpu\n"]}],"source":["!pip install timm\n","import os, sys, math, random, cv2, copy, logging, time\n","import numpy as np\n","from PIL import Image\n","from typing import Tuple, Optional, List, Union, Any\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.optim import lr_scheduler, AdamW\n","from torch.utils.data import Dataset\n","import torch.utils.checkpoint as checkpoint\n","import torchvision\n","from torchvision import datasets, transforms\n","from torchvision.transforms import InterpolationMode\n","\n","import timm\n","from timm.models.layers import to_2tuple\n","from timm.scheduler.cosine_lr import CosineLRScheduler\n","from timm.scheduler.step_lr import StepLRScheduler\n","from timm.loss import SoftTargetCrossEntropy\n","from timm.data import Mixup\n","from timm.data import create_transform\n","\n","\n","print(\"PyTorch Version: \", torch.__version__)\n","print(\"Torchvision Version: \", torchvision.__version__)\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","print(device)"]},{"cell_type":"markdown","source":["Swin Transformer model parts."],"metadata":{"id":"13QaPUU6pn-p"}},{"cell_type":"code","source":["class FeedForward(nn.Sequential):\n","    \"\"\"\n","    Feed forward module used in the transformer encoder.\n","    \"\"\"\n","\n","    def __init__(self,\n","                 in_features: int,\n","                 hidden_features: int,\n","                 out_features: int,\n","                 dropout: float = 0.) -> None:\n","        \"\"\"\n","        Constructor method\n","        :param in_features: (int) Number of input features\n","        :param hidden_features: (int) Number of hidden features\n","        :param out_features: (int) Number of output features\n","        :param dropout: (float) Dropout factor\n","        \"\"\"\n","        # Call super constructor and init modules\n","        super().__init__(\n","            nn.Linear(in_features=in_features, out_features=hidden_features),\n","            nn.GELU(),\n","            nn.Dropout(p=dropout),\n","            nn.Linear(in_features=hidden_features, out_features=out_features),\n","            nn.Dropout(p=dropout)\n","        )\n","\n","\n","def bchw_to_bhwc(input: torch.Tensor) -> torch.Tensor:\n","    \"\"\"\n","    Permutes a tensor to the shape [batch size, height, width, channels]\n","    :param input: (torch.Tensor) Input tensor of the shape [batch size, height, width, channels]\n","    :return: (torch.Tensor) Output tensor of the shape [batch size, height, width, channels]\n","    \"\"\"\n","    return input.permute(0, 2, 3, 1)\n","\n","\n","def bhwc_to_bchw(input: torch.Tensor) -> torch.Tensor:\n","    \"\"\"\n","    Permutes a tensor to the shape [batch size, channels, height, width]\n","    :param input: (torch.Tensor) Input tensor of the shape [batch size, height, width, channels]\n","    :return: (torch.Tensor) Output tensor of the shape [batch size, channels, height, width]\n","    \"\"\"\n","    return input.permute(0, 3, 1, 2)\n","\n","\n","def unfold(input: torch.Tensor,\n","           window_size: int) -> torch.Tensor:\n","    \"\"\"\n","    Unfolds (non-overlapping) a given feature map by the given window size (stride = window size)\n","    :param input: (torch.Tensor) Input feature map of the shape [batch size, channels, height, width]\n","    :param window_size: (int) Window size to be applied\n","    :return: (torch.Tensor) Unfolded tensor of the shape [batch size * windows, channels, window size, window size]\n","    \"\"\"\n","    # Get original shape\n","    _, channels, height, width = input.shape  # type: int, int, int, int\n","    # Unfold input\n","    output: torch.Tensor = input.unfold(dimension=3, size=window_size, step=window_size) \\\n","        .unfold(dimension=2, size=window_size, step=window_size)\n","    # Reshape to [batch size * windows, channels, window size, window size]\n","    output: torch.Tensor = output.permute(0, 2, 3, 1, 5, 4).reshape(-1, channels, window_size, window_size)\n","    return output\n","\n","\n","def fold(input: torch.Tensor,\n","         window_size: int,\n","         height: int,\n","         width: int) -> torch.Tensor:\n","    \"\"\"\n","    Fold a tensor of windows again to a 4D feature map\n","    :param input: (torch.Tensor) Input tensor of windows [batch size * windows, channels, window size, window size]\n","    :param window_size: (int) Window size to be reversed\n","    :param height: (int) Height of the feature map\n","    :param width: (int) Width of the feature map\n","    :return: (torch.Tensor) Folded output tensor of the shape [batch size, channels, height, width]\n","    \"\"\"\n","    # Get channels of windows\n","    channels: int = input.shape[1]\n","    # Get original batch size\n","    batch_size: int = int(input.shape[0] // (height * width // window_size // window_size))\n","    # Reshape input to\n","    output: torch.Tensor = input.view(batch_size, height // window_size, width // window_size, channels,\n","                                      window_size, window_size)\n","    output: torch.Tensor = output.permute(0, 3, 1, 4, 2, 5).reshape(batch_size, channels, height, width)\n","    return output\n","\n","\n","class WindowMultiHeadAttention(nn.Module):\n","    \"\"\"\n","    This class implements window-based Multi-Head-Attention.\n","    \"\"\"\n","\n","    def __init__(self,\n","                 in_features: int,\n","                 window_size: int,\n","                 number_of_heads: int,\n","                 dropout_attention: float = 0.,\n","                 dropout_projection: float = 0.,\n","                 meta_network_hidden_features: int = 256,\n","                 sequential_self_attention: bool = False) -> None:\n","        \"\"\"\n","        Constructor method\n","        :param in_features: (int) Number of input features\n","        :param window_size: (int) Window size\n","        :param number_of_heads: (int) Number of attention heads\n","        :param dropout_attention: (float) Dropout rate of attention map\n","        :param dropout_projection: (float) Dropout rate after projection\n","        :param meta_network_hidden_features: (int) Number of hidden features in the two layer MLP meta network\n","        :param sequential_self_attention: (bool) If true sequential self-attention is performed\n","        \"\"\"\n","        # Call super constructor\n","        super(WindowMultiHeadAttention, self).__init__()\n","        # Check parameter\n","        assert (in_features % number_of_heads) == 0, \\\n","            \"The number of input features (in_features) are not divisible by the number of heads (number_of_heads).\"\n","        # Save parameters\n","        self.in_features: int = in_features\n","        self.window_size: int = window_size\n","        self.number_of_heads: int = number_of_heads\n","        self.sequential_self_attention: bool = sequential_self_attention\n","        # Init query, key and value mapping as a single layer\n","        self.mapping_qkv: nn.Module = nn.Linear(in_features=in_features, out_features=in_features * 3, bias=True)\n","        # Init attention dropout\n","        self.attention_dropout: nn.Module = nn.Dropout(dropout_attention)\n","        # Init projection mapping\n","        self.projection: nn.Module = nn.Linear(in_features=in_features, out_features=in_features, bias=True)\n","        # Init projection dropout\n","        self.projection_dropout: nn.Module = nn.Dropout(dropout_projection)\n","        # Init meta network for positional encodings\n","        self.meta_network: nn.Module = nn.Sequential(\n","            nn.Linear(in_features=2, out_features=meta_network_hidden_features, bias=True),\n","            nn.ReLU(inplace=True),\n","            nn.Linear(in_features=meta_network_hidden_features, out_features=number_of_heads, bias=True))\n","        # Init tau\n","        self.register_parameter(\"tau\", torch.nn.Parameter(torch.ones(1, number_of_heads, 1, 1)))\n","        # Init pair-wise relative positions (log-spaced)\n","        self.__make_pair_wise_relative_positions()\n","\n","    def __make_pair_wise_relative_positions(self) -> None:\n","        \"\"\"\n","        Method initializes the pair-wise relative positions to compute the positional biases\n","        \"\"\"\n","        indexes: torch.Tensor = torch.arange(self.window_size, device=self.tau.device)\n","        coordinates: torch.Tensor = torch.stack(torch.meshgrid([indexes, indexes]), dim=0)\n","        coordinates: torch.Tensor = torch.flatten(coordinates, start_dim=1)\n","        relative_coordinates: torch.Tensor = coordinates[:, :, None] - coordinates[:, None, :]\n","        relative_coordinates: torch.Tensor = relative_coordinates.permute(1, 2, 0).reshape(-1, 2).float()\n","        relative_coordinates_log: torch.Tensor = torch.sign(relative_coordinates) \\\n","                                                 * torch.log(1. + relative_coordinates.abs())\n","        self.register_buffer(\"relative_coordinates_log\", relative_coordinates_log)\n","\n","    def update_resolution(self,\n","                          new_window_size: int,\n","                          **kwargs: Any) -> None:\n","        \"\"\"\n","        Method updates the window size and so the pair-wise relative positions\n","        :param new_window_size: (int) New window size\n","        :param kwargs: (Any) Unused\n","        \"\"\"\n","        # Set new window size\n","        self.window_size: int = new_window_size\n","        # Make new pair-wise relative positions\n","        self.__make_pair_wise_relative_positions()\n","\n","    def __get_relative_positional_encodings(self) -> torch.Tensor:\n","        \"\"\"\n","        Method computes the relative positional encodings\n","        :return: (torch.Tensor) Relative positional encodings [1, number of heads, window size ** 2, window size ** 2]\n","        \"\"\"\n","        relative_position_bias: torch.Tensor = self.meta_network(self.relative_coordinates_log)\n","        relative_position_bias: torch.Tensor = relative_position_bias.permute(1, 0)\n","        relative_position_bias: torch.Tensor = relative_position_bias.reshape(self.number_of_heads,\n","                                                                              self.window_size * self.window_size,\n","                                                                              self.window_size * self.window_size)\n","        return relative_position_bias.unsqueeze(0)\n","\n","    def __self_attention(self,\n","                         query: torch.Tensor,\n","                         key: torch.Tensor,\n","                         value: torch.Tensor,\n","                         batch_size_windows: int,\n","                         tokens: int,\n","                         mask: Optional[torch.Tensor] = None) -> torch.Tensor:\n","        \"\"\"\n","        This function performs standard (non-sequential) scaled cosine self-attention\n","        :param query: (torch.Tensor) Query tensor of the shape [batch size * windows, heads, tokens, channels // heads]\n","        :param key: (torch.Tensor) Key tensor of the shape [batch size * windows, heads, tokens, channels // heads]\n","        :param value: (torch.Tensor) Value tensor of the shape [batch size * windows, heads, tokens, channels // heads]\n","        :param batch_size_windows: (int) Size of the first dimension of the input tensor (batch size * windows)\n","        :param tokens: (int) Number of tokens in the input\n","        :param mask: (Optional[torch.Tensor]) Attention mask for the shift case\n","        :return: (torch.Tensor) Output feature map of the shape [batch size * windows, tokens, channels]\n","        \"\"\"\n","        # Compute attention map with scaled cosine attention\n","        attention_map: torch.Tensor = torch.einsum(\"bhqd, bhkd -> bhqk\", query, key) \\\n","                                      / torch.maximum(torch.norm(query, dim=-1, keepdim=True)\n","                                                      * torch.norm(key, dim=-1, keepdim=True).transpose(-2, -1),\n","                                                      torch.tensor(1e-06, device=query.device, dtype=query.dtype))\n","        attention_map: torch.Tensor = attention_map / self.tau.clamp(min=0.01)\n","        # Apply relative positional encodings\n","        attention_map: torch.Tensor = attention_map + self.__get_relative_positional_encodings()\n","        # Apply mask if utilized\n","        if mask is not None:\n","            number_of_windows: int = mask.shape[0]\n","            attention_map: torch.Tensor = attention_map.view(batch_size_windows // number_of_windows, number_of_windows,\n","                                                             self.number_of_heads, tokens, tokens)\n","            attention_map: torch.Tensor = attention_map + mask.unsqueeze(1).unsqueeze(0)\n","            attention_map: torch.Tensor = attention_map.view(-1, self.number_of_heads, tokens, tokens)\n","        attention_map: torch.Tensor = attention_map.softmax(dim=-1)\n","        # Perform attention dropout\n","        attention_map: torch.Tensor = self.attention_dropout(attention_map)\n","        # Apply attention map and reshape\n","        output: torch.Tensor = torch.einsum(\"bhal, bhlv -> bhav\", attention_map, value)\n","        output: torch.Tensor = output.permute(0, 2, 1, 3).reshape(batch_size_windows, tokens, -1)\n","        return output\n","\n","    def __sequential_self_attention(self,\n","                                    query: torch.Tensor,\n","                                    key: torch.Tensor,\n","                                    value: torch.Tensor,\n","                                    batch_size_windows: int,\n","                                    tokens: int,\n","                                    mask: Optional[torch.Tensor] = None) -> torch.Tensor:\n","        \"\"\"\n","        This function performs sequential scaled cosine self-attention\n","        :param query: (torch.Tensor) Query tensor of the shape [batch size * windows, heads, tokens, channels // heads]\n","        :param key: (torch.Tensor) Key tensor of the shape [batch size * windows, heads, tokens, channels // heads]\n","        :param value: (torch.Tensor) Value tensor of the shape [batch size * windows, heads, tokens, channels // heads]\n","        :param batch_size_windows: (int) Size of the first dimension of the input tensor (batch size * windows)\n","        :param tokens: (int) Number of tokens in the input\n","        :param mask: (Optional[torch.Tensor]) Attention mask for the shift case\n","        :return: (torch.Tensor) Output feature map of the shape [batch size * windows, tokens, channels]\n","        \"\"\"\n","        # Init output tensor\n","        output: torch.Tensor = torch.ones_like(query)\n","        # Compute relative positional encodings fist\n","        relative_position_bias: torch.Tensor = self.__get_relative_positional_encodings()\n","        # Iterate over query and key tokens\n","        for token_index_query in range(tokens):\n","            # Compute attention map with scaled cosine attention\n","            attention_map: torch.Tensor = \\\n","                torch.einsum(\"bhd, bhkd -> bhk\", query[:, :, token_index_query], key) \\\n","                / torch.maximum(torch.norm(query[:, :, token_index_query], dim=-1, keepdim=True)\n","                                * torch.norm(key, dim=-1, keepdim=False),\n","                                torch.tensor(1e-06, device=query.device, dtype=query.dtype))\n","            attention_map: torch.Tensor = attention_map / self.tau.clamp(min=0.01)[..., 0]\n","            # Apply positional encodings\n","            attention_map: torch.Tensor = attention_map + relative_position_bias[..., token_index_query, :]\n","            # Apply mask if utilized\n","            if mask is not None:\n","                number_of_windows: int = mask.shape[0]\n","                attention_map: torch.Tensor = attention_map.view(batch_size_windows // number_of_windows,\n","                                                                 number_of_windows, self.number_of_heads, 1,\n","                                                                 tokens)\n","                attention_map: torch.Tensor = attention_map \\\n","                                              + mask.unsqueeze(1).unsqueeze(0)[..., token_index_query, :].unsqueeze(3)\n","                attention_map: torch.Tensor = attention_map.view(-1, self.number_of_heads, tokens)\n","            attention_map: torch.Tensor = attention_map.softmax(dim=-1)\n","            # Perform attention dropout\n","            attention_map: torch.Tensor = self.attention_dropout(attention_map)\n","            # Apply attention map and reshape\n","            output[:, :, token_index_query] = torch.einsum(\"bhl, bhlv -> bhv\", attention_map, value)\n","        output: torch.Tensor = output.permute(0, 2, 1, 3).reshape(batch_size_windows, tokens, -1)\n","        return output\n","\n","    def forward(self,\n","                input: torch.Tensor,\n","                mask: Optional[torch.Tensor] = None) -> torch.Tensor:\n","        \"\"\"\n","        Forward pass\n","        :param input: (torch.Tensor) Input tensor of the shape [batch size * windows, channels, height, width]\n","        :param mask: (Optional[torch.Tensor]) Attention mask for the shift case\n","        :return: (torch.Tensor) Output tensor of the shape [batch size * windows, channels, height, width]\n","        \"\"\"\n","        # Save original shape\n","        batch_size_windows, channels, height, width = input.shape  # type: int, int, int, int\n","        tokens: int = height * width\n","        # Reshape input to [batch size * windows, tokens (height * width), channels]\n","        input: torch.Tensor = input.reshape(batch_size_windows, channels, tokens).permute(0, 2, 1)\n","        # Perform query, key, and value mapping\n","        query_key_value: torch.Tensor = self.mapping_qkv(input)\n","        query_key_value: torch.Tensor = query_key_value.view(batch_size_windows, tokens, 3, self.number_of_heads,\n","                                                             channels // self.number_of_heads).permute(2, 0, 3, 1, 4)\n","        query, key, value = query_key_value[0], query_key_value[1], query_key_value[2]\n","        # Perform attention\n","        if self.sequential_self_attention:\n","            output: torch.Tensor = self.__sequential_self_attention(query=query, key=key, value=value,\n","                                                                    batch_size_windows=batch_size_windows,\n","                                                                    tokens=tokens,\n","                                                                    mask=mask)\n","        else:\n","            output: torch.Tensor = self.__self_attention(query=query, key=key, value=value,\n","                                                         batch_size_windows=batch_size_windows, tokens=tokens,\n","                                                         mask=mask)\n","        # Perform linear mapping and dropout\n","        output: torch.Tensor = self.projection_dropout(self.projection(output))\n","        # Reshape output to original shape [batch size * windows, channels, height, width]\n","        output: torch.Tensor = output.permute(0, 2, 1).view(batch_size_windows, channels, height, width)\n","        return output\n","\n","\n","class SwinTransformerBlock(nn.Module):\n","    \"\"\"\n","    This class implements the Swin transformer block.\n","    \"\"\"\n","\n","    def __init__(self,\n","                 in_channels: int,\n","                 input_resolution: Tuple[int, int],\n","                 number_of_heads: int,\n","                 window_size: int = 7,\n","                 shift_size: int = 0,\n","                 ff_feature_ratio: int = 4,\n","                 dropout: float = 0.0,\n","                 dropout_attention: float = 0.0,\n","                 dropout_path: float = 0.0,\n","                 sequential_self_attention: bool = False) -> None:\n","        \"\"\"\n","        Constructor method\n","        :param in_channels: (int) Number of input channels\n","        :param input_resolution: (Tuple[int, int]) Input resolution\n","        :param number_of_heads: (int) Number of attention heads to be utilized\n","        :param window_size: (int) Window size to be utilized\n","        :param shift_size: (int) Shifting size to be used\n","        :param ff_feature_ratio: (int) Ratio of the hidden dimension in the FFN to the input channels\n","        :param dropout: (float) Dropout in input mapping\n","        :param dropout_attention: (float) Dropout rate of attention map\n","        :param dropout_path: (float) Dropout in main path\n","        :param sequential_self_attention: (bool) If true sequential self-attention is performed\n","        \"\"\"\n","        # Call super constructor\n","        super(SwinTransformerBlock, self).__init__()\n","        # Save parameters\n","        self.in_channels: int = in_channels\n","        self.input_resolution: Tuple[int, int] = input_resolution\n","        # Catch case if resolution is smaller than the window size\n","        if min(self.input_resolution) <= window_size:\n","            self.window_size: int = min(self.input_resolution)\n","            self.shift_size: int = 0\n","            self.make_windows: bool = False\n","        else:\n","            self.window_size: int = window_size\n","            self.shift_size: int = shift_size\n","            self.make_windows: bool = True\n","        # Init normalization layers\n","        self.normalization_1: nn.Module = nn.LayerNorm(normalized_shape=in_channels)\n","        self.normalization_2: nn.Module = nn.LayerNorm(normalized_shape=in_channels)\n","        # Init window attention module\n","        self.window_attention: WindowMultiHeadAttention = WindowMultiHeadAttention(\n","            in_features=in_channels,\n","            window_size=self.window_size,\n","            number_of_heads=number_of_heads,\n","            dropout_attention=dropout_attention,\n","            dropout_projection=dropout,\n","            sequential_self_attention=sequential_self_attention)\n","        # Init dropout layer\n","        self.dropout: nn.Module = timm.models.layers.DropPath(\n","            drop_prob=dropout_path) if dropout_path > 0. else nn.Identity()\n","        # Init feed-forward network\n","        self.feed_forward_network: nn.Module = FeedForward(in_features=in_channels,\n","                                                           hidden_features=int(in_channels * ff_feature_ratio),\n","                                                           dropout=dropout,\n","                                                           out_features=in_channels)\n","        # Make attention mask\n","        self.__make_attention_mask()\n","\n","    def __make_attention_mask(self) -> None:\n","        \"\"\"\n","        Method generates the attention mask used in shift case\n","        \"\"\"\n","        # Make masks for shift case\n","        if self.shift_size > 0:\n","            height, width = self.input_resolution  # type: int, int\n","            mask: torch.Tensor = torch.zeros(height, width, device=self.window_attention.tau.device)\n","            height_slices: Tuple = (slice(0, -self.window_size),\n","                                    slice(-self.window_size, -self.shift_size),\n","                                    slice(-self.shift_size, None))\n","            width_slices: Tuple = (slice(0, -self.window_size),\n","                                   slice(-self.window_size, -self.shift_size),\n","                                   slice(-self.shift_size, None))\n","            counter: int = 0\n","            for height_slice in height_slices:\n","                for width_slice in width_slices:\n","                    mask[height_slice, width_slice] = counter\n","                    counter += 1\n","            mask_windows: torch.Tensor = unfold(mask[None, None], self.window_size)\n","            mask_windows: torch.Tensor = mask_windows.reshape(-1, self.window_size * self.window_size)\n","            attention_mask: Optional[torch.Tensor] = mask_windows.unsqueeze(1) - mask_windows.unsqueeze(2)\n","            attention_mask: Optional[torch.Tensor] = attention_mask.masked_fill(attention_mask != 0, float(-100.0))\n","            attention_mask: Optional[torch.Tensor] = attention_mask.masked_fill(attention_mask == 0, float(0.0))\n","        else:\n","            attention_mask: Optional[torch.Tensor] = None\n","        # Save mask\n","        self.register_buffer(\"attention_mask\", attention_mask)\n","\n","    def update_resolution(self,\n","                          new_window_size: int,\n","                          new_input_resolution: Tuple[int, int]) -> None:\n","        \"\"\"\n","        Method updates the window size and so the pair-wise relative positions\n","        :param new_window_size: (int) New window size\n","        :param new_input_resolution: (Tuple[int, int]) New input resolution\n","        \"\"\"\n","        # Update input resolution\n","        self.input_resolution: Tuple[int, int] = new_input_resolution\n","        # Catch case if resolution is smaller than the window size\n","        if min(self.input_resolution) <= new_window_size:\n","            self.window_size: int = min(self.input_resolution)\n","            self.shift_size: int = 0\n","            self.make_windows: bool = False\n","        else:\n","            self.window_size: int = new_window_size\n","            self.shift_size: int = self.shift_size\n","            self.make_windows: bool = True\n","        # Update attention mask\n","        self.__make_attention_mask()\n","        # Update attention module\n","        self.window_attention.update_resolution(new_window_size=new_window_size)\n","\n","    def forward(self,\n","                input: torch.Tensor) -> torch.Tensor:\n","        \"\"\"\n","        Forward pass\n","        :param input: (torch.Tensor) Input tensor of the shape [batch size, in channels, height, width]\n","        :return: (torch.Tensor) Output tensor of the shape [batch size, in channels, height, width]\n","        \"\"\"\n","        # Save shape\n","        batch_size, channels, height, width = input.shape  # type: int, int, int, int\n","        # Shift input if utilized\n","        if self.shift_size > 0:\n","            output_shift: torch.Tensor = torch.roll(input=input, shifts=(-self.shift_size, -self.shift_size),\n","                                                    dims=(-1, -2))\n","        else:\n","            output_shift: torch.Tensor = input\n","        # Make patches\n","        output_patches: torch.Tensor = unfold(input=output_shift, window_size=self.window_size) \\\n","            if self.make_windows else output_shift\n","        # Perform window attention\n","        output_attention: torch.Tensor = self.window_attention(output_patches, mask=self.attention_mask)\n","        # Merge patches\n","        output_merge: torch.Tensor = fold(input=output_attention, window_size=self.window_size, height=height,\n","                                          width=width) if self.make_windows else output_attention\n","        # Reverse shift if utilized\n","        if self.shift_size > 0:\n","            output_shift: torch.Tensor = torch.roll(input=output_merge, shifts=(self.shift_size, self.shift_size),\n","                                                    dims=(-1, -2))\n","        else:\n","            output_shift: torch.Tensor = output_merge\n","        # Perform normalization\n","        output_normalize: torch.Tensor = self.normalization_1(output_shift.permute(0, 2, 3, 1)).permute(0, 3, 1, 2)\n","        # Skip connection\n","        output_skip: torch.Tensor = self.dropout(output_normalize) + input\n","        # Feed forward network, normalization and skip connection\n","        output_feed_forward: torch.Tensor = self.feed_forward_network(\n","            output_skip.view(batch_size, channels, -1).permute(0, 2, 1)).permute(0, 2, 1)\n","        output_feed_forward: torch.Tensor = output_feed_forward.view(batch_size, channels, height, width)\n","        output_normalize: torch.Tensor = bhwc_to_bchw(self.normalization_2(bchw_to_bhwc(output_feed_forward)))\n","        output: torch.Tensor = output_skip + self.dropout(output_normalize)\n","        return output\n","\n","\n","class DeformableSwinTransformerBlock(SwinTransformerBlock):\n","    \"\"\"\n","    This class implements a deformable version of the Swin Transformer block.\n","    Inspired by: https://arxiv.org/pdf/2201.00520.pdf\n","    \"\"\"\n","\n","    def __init__(self,\n","                 in_channels: int,\n","                 input_resolution: Tuple[int, int],\n","                 number_of_heads: int,\n","                 window_size: int = 7,\n","                 shift_size: int = 0,\n","                 ff_feature_ratio: int = 4,\n","                 dropout: float = 0.0,\n","                 dropout_attention: float = 0.0,\n","                 dropout_path: float = 0.0,\n","                 sequential_self_attention: bool = False,\n","                 offset_downscale_factor: int = 2) -> None:\n","        \"\"\"\n","        Constructor method\n","        :param in_channels: (int) Number of input channels\n","        :param input_resolution: (Tuple[int, int]) Input resolution\n","        :param number_of_heads: (int) Number of attention heads to be utilized\n","        :param window_size: (int) Window size to be utilized\n","        :param shift_size: (int) Shifting size to be used\n","        :param ff_feature_ratio: (int) Ratio of the hidden dimension in the FFN to the input channels\n","        :param dropout: (float) Dropout in input mapping\n","        :param dropout_attention: (float) Dropout rate of attention map\n","        :param dropout_path: (float) Dropout in main path\n","        :param sequential_self_attention: (bool) If true sequential self-attention is performed\n","        :param offset_downscale_factor: (int) Downscale factor of offset network\n","        \"\"\"\n","        # Call super constructor\n","        super(DeformableSwinTransformerBlock, self).__init__(\n","            in_channels=in_channels,\n","            input_resolution=input_resolution,\n","            number_of_heads=number_of_heads,\n","            window_size=window_size,\n","            shift_size=shift_size,\n","            ff_feature_ratio=ff_feature_ratio,\n","            dropout=dropout,\n","            dropout_attention=dropout_attention,\n","            dropout_path=dropout_path,\n","            sequential_self_attention=sequential_self_attention\n","        )\n","        # Save parameter\n","        self.offset_downscale_factor: int = offset_downscale_factor\n","        self.number_of_heads: int = number_of_heads\n","        # Make default offsets\n","        self.__make_default_offsets()\n","        # Init offset network\n","        self.offset_network: nn.Module = nn.Sequential(\n","            nn.Conv2d(in_channels=in_channels, out_channels=in_channels, kernel_size=5, stride=offset_downscale_factor,\n","                      padding=3, groups=in_channels, bias=True),\n","            nn.GELU(),\n","            nn.Conv2d(in_channels=in_channels, out_channels=2 * self.number_of_heads, kernel_size=1, stride=1,\n","                      padding=0, bias=True)\n","        )\n","\n","    def __make_default_offsets(self) -> None:\n","        \"\"\"\n","        Method generates the default sampling grid (inspired by kornia)\n","        \"\"\"\n","        # Init x and y coordinates\n","        x: torch.Tensor = torch.linspace(0, self.input_resolution[1] - 1, self.input_resolution[1],\n","                                         device=self.window_attention.tau.device)\n","        y: torch.Tensor = torch.linspace(0, self.input_resolution[0] - 1, self.input_resolution[0],\n","                                         device=self.window_attention.tau.device)\n","        # Normalize coordinates to a range of [-1, 1]\n","        x: torch.Tensor = (x / (self.input_resolution[1] - 1) - 0.5) * 2\n","        y: torch.Tensor = (y / (self.input_resolution[0] - 1) - 0.5) * 2\n","        # Make grid [2, height, width]\n","        grid: torch.Tensor = torch.stack(torch.meshgrid([x, y])).transpose(1, 2)\n","        # Reshape grid to [1, height, width, 2]\n","        grid: torch.Tensor = grid.unsqueeze(dim=0).permute(0, 2, 3, 1)\n","        # Register in module\n","        self.register_buffer(\"default_grid\", grid)\n","\n","    def update_resolution(self, new_window_size: int, new_input_resolution: Tuple[int, int]) -> None:\n","        \"\"\"\n","        Method updates the window size and so the pair-wise relative positions\n","        :param new_window_size: (int) New window size\n","        :param new_input_resolution: (Tuple[int, int]) New input resolution\n","        \"\"\"\n","        # Update resolution and window size\n","        super(DeformableSwinTransformerBlock, self).update_resolution(new_window_size=new_window_size,\n","                                                                      new_input_resolution=new_input_resolution)\n","        # Update default sampling grid\n","        self.__make_default_offsets()\n","\n","    def forward(self,\n","                input: torch.Tensor) -> torch.Tensor:\n","        # Get input shape\n","        batch_size, channels, height, width = input.shape\n","        # Compute offsets of the shape [batch size, 2, height / r, width / r]\n","        offsets: torch.Tensor = self.offset_network(input)\n","        # Upscale offsets to the shape [batch size, 2 * number of heads, height, width]\n","        offsets: torch.Tensor = F.interpolate(input=offsets,\n","                                              size=(height, width), mode=\"bilinear\", align_corners=True)\n","        # Reshape offsets to [batch size, number of heads, height, width, 2]\n","        offsets: torch.Tensor = offsets.reshape(batch_size, -1, 2, height, width).permute(0, 1, 3, 4, 2)\n","        # Flatten batch size and number of heads and apply tanh\n","        offsets: torch.Tensor = offsets.view(-1, height, width, 2).tanh()\n","        # Cast offset grid to input data type\n","        if input.dtype != self.default_grid.dtype:\n","            self.default_grid = self.default_grid.type(input.dtype)\n","        # Construct offset grid\n","        offset_grid: torch.Tensor = self.default_grid.repeat_interleave(repeats=offsets.shape[0], dim=0) + offsets\n","        # Reshape input to [batch size * number of heads, channels / number of heads, height, width]\n","        input: torch.Tensor = input.view(batch_size, self.number_of_heads, channels // self.number_of_heads, height,\n","                                         width).flatten(start_dim=0, end_dim=1)\n","        # Apply sampling grid\n","        input_resampled: torch.Tensor = F.grid_sample(input=input, grid=offset_grid.clip(min=-1, max=1),\n","                                                      mode=\"bilinear\", align_corners=True, padding_mode=\"reflection\")\n","        # Reshape resampled tensor again to [batch size, channels, height, width]\n","        input_resampled: torch.Tensor = input_resampled.view(batch_size, channels, height, width)\n","        return super(DeformableSwinTransformerBlock, self).forward(input=input_resampled)\n","\n","\n","class PatchMerging(nn.Module):\n","    \"\"\"\n","    This class implements the patch merging approach which is essential a strided convolution with normalization before\n","    \"\"\"\n","\n","    def __init__(self,\n","                 in_channels: int) -> None:\n","        \"\"\"\n","        Constructor method\n","        :param in_channels: (int) Number of input channels\n","        \"\"\"\n","        # Call super constructor\n","        super(PatchMerging, self).__init__()\n","        # Init normalization\n","        self.normalization: nn.Module = nn.LayerNorm(normalized_shape=4 * in_channels)\n","        # Init linear mapping\n","        self.linear_mapping: nn.Module = nn.Linear(in_features=4 * in_channels, out_features=2 * in_channels,\n","                                                   bias=False)\n","\n","    def forward(self,\n","                input: torch.Tensor) -> torch.Tensor:\n","        \"\"\"\n","        Forward pass\n","        :param input: (torch.Tensor) Input tensor of the shape [batch size, in channels, height, width]\n","        :return: (torch.Tensor) Output tensor of the shape [batch size, 2 * in channels, height // 2, width // 2]\n","        \"\"\"\n","        # Get original shape\n","        batch_size, channels, height, width = input.shape  # type: int, int, int, int\n","        # Reshape input to [batch size, in channels, height, width]\n","        input: torch.Tensor = bchw_to_bhwc(input)\n","        # Unfold input\n","        input: torch.Tensor = input.unfold(dimension=1, size=2, step=2).unfold(dimension=2, size=2, step=2)\n","        input: torch.Tensor = input.reshape(batch_size, input.shape[1], input.shape[2], -1)\n","        # Normalize input\n","        input: torch.Tensor = self.normalization(input)\n","        # Perform linear mapping\n","        output: torch.Tensor = bhwc_to_bchw(self.linear_mapping(input))\n","        return output\n","\n","\n","class PatchEmbedding(nn.Module):\n","    \"\"\"\n","    Module embeds a given image into patch embeddings.\n","    \"\"\"\n","\n","    def __init__(self,\n","                 in_channels: int = 3,\n","                 out_channels: int = 96,\n","                 patch_size: int = 4) -> None:\n","        \"\"\"\n","        Constructor method\n","        :param in_channels: (int) Number of input channels\n","        :param out_channels: (int) Number of output channels\n","        :param patch_size: (int) Patch size to be utilized\n","        :param image_size: (int) Image size to be used\n","        \"\"\"\n","        # Call super constructor\n","        super(PatchEmbedding, self).__init__()\n","        # Save parameters\n","        self.out_channels: int = out_channels\n","        # Init linear embedding as a convolution\n","        self.linear_embedding: nn.Module = nn.Conv2d(in_channels=in_channels, out_channels=out_channels,\n","                                                     kernel_size=(patch_size, patch_size),\n","                                                     stride=(patch_size, patch_size))\n","        # Init layer normalization\n","        self.normalization: nn.Module = nn.LayerNorm(normalized_shape=out_channels)\n","\n","    def forward(self, input: torch.Tensor) -> torch.Tensor:\n","        \"\"\"\n","        Forward pass transforms a given batch of images into a patch embedding\n","        :param input: (torch.Tensor) Input images of the shape [batch size, in channels, height, width]\n","        :return: (torch.Tensor) Patch embedding of the shape [batch size, patches + 1, out channels]\n","        \"\"\"\n","        # Perform linear embedding\n","        embedding: torch.Tensor = self.linear_embedding(input)\n","        # Perform normalization\n","        embedding: torch.Tensor = bhwc_to_bchw(self.normalization(bchw_to_bhwc(embedding)))\n","        return embedding\n","\n","\n","class SwinTransformerStage(nn.Module):\n","    \"\"\"\n","    This class implements a stage of the Swin transformer including multiple layers.\n","    \"\"\"\n","\n","    def __init__(self,\n","                 in_channels: int,\n","                 depth: int,\n","                 downscale: bool,\n","                 input_resolution: Tuple[int, int],\n","                 number_of_heads: int,\n","                 window_size: int = 7,\n","                 ff_feature_ratio: int = 4,\n","                 dropout: float = 0.0,\n","                 dropout_attention: float = 0.0,\n","                 dropout_path: Union[List[float], float] = 0.0,\n","                 use_checkpoint: bool = False,\n","                 sequential_self_attention: bool = False,\n","                 use_deformable_block: bool = False) -> None:\n","        \"\"\"\n","        Constructor method\n","        :param in_channels: (int) Number of input channels\n","        :param depth: (int) Depth of the stage (number of layers)\n","        :param downscale: (bool) If true input is downsampled (see Fig. 3 or V1 paper)\n","        :param input_resolution: (Tuple[int, int]) Input resolution\n","        :param number_of_heads: (int) Number of attention heads to be utilized\n","        :param window_size: (int) Window size to be utilized\n","        :param shift_size: (int) Shifting size to be used\n","        :param ff_feature_ratio: (int) Ratio of the hidden dimension in the FFN to the input channels\n","        :param dropout: (float) Dropout in input mapping\n","        :param dropout_attention: (float) Dropout rate of attention map\n","        :param dropout_path: (float) Dropout in main path\n","        :param use_checkpoint: (bool) If true checkpointing is utilized\n","        :param sequential_self_attention: (bool) If true sequential self-attention is performed\n","        :param use_deformable_block: (bool) If true deformable block is used\n","        \"\"\"\n","        # Call super constructor\n","        super(SwinTransformerStage, self).__init__()\n","        # Save parameters\n","        self.use_checkpoint: bool = use_checkpoint\n","        self.downscale: bool = downscale\n","        # Init downsampling\n","        self.downsample: nn.Module = PatchMerging(in_channels=in_channels) if downscale else nn.Identity()\n","        # Update resolution and channels\n","        self.input_resolution: Tuple[int, int] = (input_resolution[0] // 2, input_resolution[1] // 2) \\\n","            if downscale else input_resolution\n","        in_channels = in_channels * 2 if downscale else in_channels\n","        # Get block\n","        block = DeformableSwinTransformerBlock if use_deformable_block else SwinTransformerBlock\n","        # Init blocks\n","        self.blocks: nn.ModuleList = nn.ModuleList([\n","            block(in_channels=in_channels,\n","                  input_resolution=self.input_resolution,\n","                  number_of_heads=number_of_heads,\n","                  window_size=window_size,\n","                  shift_size=0 if ((index % 2) == 0) else window_size // 2,\n","                  ff_feature_ratio=ff_feature_ratio,\n","                  dropout=dropout,\n","                  dropout_attention=dropout_attention,\n","                  dropout_path=dropout_path[index] if isinstance(dropout_path, list) else dropout_path,\n","                  sequential_self_attention=sequential_self_attention)\n","            for index in range(depth)])\n","\n","    def update_resolution(self, new_window_size: int, new_input_resolution: Tuple[int, int]) -> None:\n","        \"\"\"\n","        Method updates the window size and so the pair-wise relative positions\n","        :param new_window_size: (int) New window size\n","        :param new_input_resolution: (Tuple[int, int]) New input resolution\n","        \"\"\"\n","        # Update resolution\n","        self.input_resolution: Tuple[int, int] = (new_input_resolution[0] // 2, new_input_resolution[1] // 2) \\\n","            if self.downscale else new_input_resolution\n","        # Update resolution of each block\n","        for block in self.blocks:  # type: SwinTransformerBlock\n","            block.update_resolution(new_window_size=new_window_size, new_input_resolution=self.input_resolution)\n","\n","    def forward(self, input: torch.Tensor) -> torch.Tensor:\n","        \"\"\"\n","        Forward pass\n","        :param input: (torch.Tensor) Input tensor of the shape [batch size, channels, height, width]\n","        :return: (torch.Tensor) Output tensor of the shape [batch size, 2 * channels, height // 2, width // 2]\n","        \"\"\"\n","        # Downscale input tensor\n","        output: torch.Tensor = self.downsample(input)\n","        # Forward pass of each block\n","        for block in self.blocks:  # type: nn.Module\n","            # Perform checkpointing if utilized\n","            if self.use_checkpoint:\n","                output: torch.Tensor = checkpoint.checkpoint(block, output)\n","            else:\n","                output: torch.Tensor = block(output)\n","        return output\n"],"metadata":{"id":"tqM-szCrpuNE"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Combine parts to form Swin Transformer."],"metadata":{"id":"q5cXKQTCp24p"}},{"cell_type":"code","source":["class SwinTransformerV2(nn.Module):\n","    \"\"\"\n","    This class implements the Swin Transformer without classification head.\n","    \"\"\"\n","\n","    def __init__(self,\n","                 in_channels: int,\n","                 embedding_channels: int,\n","                 depths: Tuple[int, ...],\n","                 input_resolution: Tuple[int, int],\n","                 number_of_heads: Tuple[int, ...],\n","                 window_size: int = 7,\n","                 patch_size: int = 4,\n","                 ff_feature_ratio: int = 4,\n","                 dropout: float = 0.0,\n","                 dropout_attention: float = 0.0,\n","                 dropout_path: float = 0.2,\n","                 use_checkpoint: bool = False,\n","                 sequential_self_attention: bool = False,\n","                 use_deformable_block: bool = False) -> None:\n","        \"\"\"\n","        Constructor method\n","        :param in_channels: (int) Number of input channels\n","        :param depth: (int) Depth of the stage (number of layers)\n","        :param downscale: (bool) If true input is downsampled (see Fig. 3 or V1 paper)\n","        :param input_resolution: (Tuple[int, int]) Input resolution\n","        :param number_of_heads: (int) Number of attention heads to be utilized\n","        :param window_size: (int) Window size to be utilized\n","        :param shift_size: (int) Shifting size to be used\n","        :param ff_feature_ratio: (int) Ratio of the hidden dimension in the FFN to the input channels\n","        :param dropout: (float) Dropout in input mapping\n","        :param dropout_attention: (float) Dropout rate of attention map\n","        :param dropout_path: (float) Dropout in main path\n","        :param use_checkpoint: (bool) If true checkpointing is utilized\n","        :param sequential_self_attention: (bool) If true sequential self-attention is performed\n","        :param use_deformable_block: (bool) If true deformable block is used\n","        \"\"\"\n","        # Call super constructor\n","        super(SwinTransformerV2, self).__init__()\n","        # Save parameters\n","        self.patch_size: int = patch_size\n","        # Init patch embedding\n","        self.patch_embedding: nn.Module = PatchEmbedding(in_channels=in_channels, out_channels=embedding_channels,\n","                                                         patch_size=patch_size)\n","        # Compute patch resolution\n","        patch_resolution: Tuple[int, int] = (input_resolution[0] // patch_size, input_resolution[1] // patch_size)\n","        # Path dropout dependent on depth\n","        dropout_path = torch.linspace(0., dropout_path, sum(depths)).tolist()\n","        # Init stages\n","        self.stages: nn.ModuleList = nn.ModuleList()\n","        for index, (depth, number_of_head) in enumerate(zip(depths, number_of_heads)):\n","            self.stages.append(\n","                SwinTransformerStage(\n","                    in_channels=embedding_channels * (2 ** max(index - 1, 0)),\n","                    depth=depth,\n","                    downscale=not (index == 0),\n","                    input_resolution=(patch_resolution[0] // (2 ** max(index - 1, 0)),\n","                                      patch_resolution[1] // (2 ** max(index - 1, 0))),\n","                    number_of_heads=number_of_head,\n","                    window_size=window_size,\n","                    ff_feature_ratio=ff_feature_ratio,\n","                    dropout=dropout,\n","                    dropout_attention=dropout_attention,\n","                    dropout_path=dropout_path[sum(depths[:index]):sum(depths[:index + 1])],\n","                    use_checkpoint=use_checkpoint,\n","                    sequential_self_attention=sequential_self_attention,\n","                    use_deformable_block=use_deformable_block and (index > 0)\n","                ))\n","\n","    def update_resolution(self, new_window_size: int, new_input_resolution: Tuple[int, int]) -> None:\n","        \"\"\"\n","        Method updates the window size and so the pair-wise relative positions\n","        :param new_window_size: (int) New window size\n","        :param new_input_resolution: (Tuple[int, int]) New input resolution\n","        \"\"\"\n","        # Compute new patch resolution\n","        new_patch_resolution: Tuple[int, int] = (new_input_resolution[0] // self.patch_size,\n","                                                 new_input_resolution[1] // self.patch_size)\n","        # Update resolution of each stage\n","        for index, stage in enumerate(self.stages):  # type: int, SwinTransformerStage\n","            stage.update_resolution(new_window_size=new_window_size,\n","                                    new_input_resolution=(new_patch_resolution[0] // (2 ** max(index - 1, 0)),\n","                                                          new_patch_resolution[1] // (2 ** max(index - 1, 0))))\n","\n","    def forward(self, input: torch.Tensor) -> List[torch.Tensor]:\n","        \"\"\"\n","        Forward pass\n","        :param input: (torch.Tensor) Input tensor\n","        :return: (List[torch.Tensor]) List of features from each stage\n","        \"\"\"\n","        # Perform patch embedding\n","        output: torch.Tensor = self.patch_embedding(input)\n","        # Init list to store feature\n","        features: List[torch.Tensor] = []\n","        # Forward pass of each stage\n","        for stage in self.stages:\n","            output: torch.Tensor = stage(output)\n","            features.append(output)\n","        return features\n","\n","\n","def swin_transformer_v2_t(input_resolution: Tuple[int, int],\n","                          window_size: int = 7,\n","                          in_channels: int = 3,\n","                          use_checkpoint: bool = False,\n","                          sequential_self_attention: bool = False,\n","                          **kwargs) -> SwinTransformerV2:\n","    \"\"\"\n","    Function returns a tiny Swin Transformer V2 (SwinV2-T: C = 96, layer numbers = {2, 2, 6, 2}) for feature extraction\n","    :param input_resolution: (Tuple[int, int]) Input resolution\n","    :param window_size: (int) Window size to be utilized\n","    :param in_channels: (int) Number of input channels\n","    :param use_checkpoint: (bool) If true checkpointing is utilized\n","    :param sequential_self_attention: (bool) If true sequential self-attention is performed\n","    :return: (SwinTransformerV2) Tiny Swin Transformer V2\n","    \"\"\"\n","    return SwinTransformerV2(input_resolution=input_resolution,\n","                             window_size=window_size,\n","                             in_channels=in_channels,\n","                             use_checkpoint=use_checkpoint,\n","                             sequential_self_attention=sequential_self_attention,\n","                             embedding_channels=96,\n","                             depths=(2, 2, 6, 2),\n","                             number_of_heads=(3, 6, 12, 24),\n","                             **kwargs)\n","\n","\n","def swin_transformer_v2_s(input_resolution: Tuple[int, int],\n","                          window_size: int = 7,\n","                          in_channels: int = 3,\n","                          use_checkpoint: bool = False,\n","                          sequential_self_attention: bool = False,\n","                          **kwargs) -> SwinTransformerV2:\n","    \"\"\"\n","    Function returns a small Swin Transformer V2 (SwinV2-S: C = 96, layer numbers ={2, 2, 18, 2}) for feature extraction\n","    :param input_resolution: (Tuple[int, int]) Input resolution\n","    :param window_size: (int) Window size to be utilized\n","    :param in_channels: (int) Number of input channels\n","    :param use_checkpoint: (bool) If true checkpointing is utilized\n","    :param sequential_self_attention: (bool) If true sequential self-attention is performed\n","    :return: (SwinTransformerV2) Small Swin Transformer V2\n","    \"\"\"\n","    return SwinTransformerV2(input_resolution=input_resolution,\n","                             window_size=window_size,\n","                             in_channels=in_channels,\n","                             use_checkpoint=use_checkpoint,\n","                             sequential_self_attention=sequential_self_attention,\n","                             embedding_channels=96,\n","                             depths=(2, 2, 18, 2),\n","                             number_of_heads=(3, 6, 12, 24),\n","                             **kwargs)\n","\n","\n","def swin_transformer_v2_b(input_resolution: Tuple[int, int],\n","                          window_size: int = 7,\n","                          in_channels: int = 3,\n","                          use_checkpoint: bool = False,\n","                          sequential_self_attention: bool = False,\n","                          **kwargs) -> SwinTransformerV2:\n","    \"\"\"\n","    Function returns a base Swin Transformer V2 (SwinV2-B: C = 128, layer numbers ={2, 2, 18, 2}) for feature extraction\n","    :param input_resolution: (Tuple[int, int]) Input resolution\n","    :param window_size: (int) Window size to be utilized\n","    :param in_channels: (int) Number of input channels\n","    :param use_checkpoint: (bool) If true checkpointing is utilized\n","    :param sequential_self_attention: (bool) If true sequential self-attention is performed\n","    :return: (SwinTransformerV2) Base Swin Transformer V2\n","    \"\"\"\n","    return SwinTransformerV2(input_resolution=input_resolution,\n","                             window_size=window_size,\n","                             in_channels=in_channels,\n","                             use_checkpoint=use_checkpoint,\n","                             sequential_self_attention=sequential_self_attention,\n","                             embedding_channels=128,\n","                             depths=(2, 2, 18, 2),\n","                             number_of_heads=(4, 8, 16, 32),\n","                             **kwargs)\n","\n","\n","def swin_transformer_v2_l(input_resolution: Tuple[int, int],\n","                          window_size: int = 7,\n","                          in_channels: int = 3,\n","                          use_checkpoint: bool = False,\n","                          sequential_self_attention: bool = False,\n","                          **kwargs) -> SwinTransformerV2:\n","    \"\"\"\n","    Function returns a large Swin Transformer V2 (SwinV2-L: C = 192, layer numbers ={2, 2, 18, 2}) for feature extraction\n","    :param input_resolution: (Tuple[int, int]) Input resolution\n","    :param window_size: (int) Window size to be utilized\n","    :param in_channels: (int) Number of input channels\n","    :param use_checkpoint: (bool) If true checkpointing is utilized\n","    :param sequential_self_attention: (bool) If true sequential self-attention is performed\n","    :return: (SwinTransformerV2) Large Swin Transformer V2\n","    \"\"\"\n","    return SwinTransformerV2(input_resolution=input_resolution,\n","                             window_size=window_size,\n","                             in_channels=in_channels,\n","                             use_checkpoint=use_checkpoint,\n","                             sequential_self_attention=sequential_self_attention,\n","                             embedding_channels=192,\n","                             depths=(2, 2, 18, 2),\n","                             number_of_heads=(6, 12, 24, 48),\n","                             **kwargs)\n","\n","\n","def swin_transformer_v2_h(input_resolution: Tuple[int, int],\n","                          window_size: int = 7,\n","                          in_channels: int = 3,\n","                          use_checkpoint: bool = False,\n","                          sequential_self_attention: bool = False,\n","                          **kwargs) -> SwinTransformerV2:\n","    \"\"\"\n","    Function returns a large Swin Transformer V2 (SwinV2-H: C = 352, layer numbers = {2, 2, 18, 2}) for feature extraction\n","    :param input_resolution: (Tuple[int, int]) Input resolution\n","    :param window_size: (int) Window size to be utilized\n","    :param in_channels: (int) Number of input channels\n","    :param use_checkpoint: (bool) If true checkpointing is utilized\n","    :param sequential_self_attention: (bool) If true sequential self-attention is performed\n","    :return: (SwinTransformerV2) Large Swin Transformer V2\n","    \"\"\"\n","    return SwinTransformerV2(input_resolution=input_resolution,\n","                             window_size=window_size,\n","                             in_channels=in_channels,\n","                             use_checkpoint=use_checkpoint,\n","                             sequential_self_attention=sequential_self_attention,\n","                             embedding_channels=352,\n","                             depths=(2, 2, 18, 2),\n","                             number_of_heads=(11, 22, 44, 88),\n","                             **kwargs)\n","\n","\n","def swin_transformer_v2_g(input_resolution: Tuple[int, int],\n","                          window_size: int = 7,\n","                          in_channels: int = 3,\n","                          use_checkpoint: bool = False,\n","                          sequential_self_attention: bool = False,\n","                          **kwargs) -> SwinTransformerV2:\n","    \"\"\"\n","    Function returns a giant Swin Transformer V2 (SwinV2-G: C = 512, layer numbers = {2, 2, 42, 2}) for feature extraction\n","    :param input_resolution: (Tuple[int, int]) Input resolution\n","    :param window_size: (int) Window size to be utilized\n","    :param in_channels: (int) Number of input channels\n","    :param use_checkpoint: (bool) If true checkpointing is utilized\n","    :param sequential_self_attention: (bool) If true sequential self-attention is performed\n","    :return: (SwinTransformerV2) Giant Swin Transformer V2\n","    \"\"\"\n","    return SwinTransformerV2(input_resolution=input_resolution,\n","                             window_size=window_size,\n","                             in_channels=in_channels,\n","                             use_checkpoint=use_checkpoint,\n","                             sequential_self_attention=sequential_self_attention,\n","                             embedding_channels=512,\n","                             depths=(2, 2, 42, 2),\n","                             number_of_heads=(16, 32, 64, 128),\n","                             **kwargs)\n"],"metadata":{"id":"JCRO8ltUp7jH"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Train"],"metadata":{"id":"WuOC3dtnQc3K"}},{"cell_type":"code","source":["def main() -> None:\n","    # Check for cuda and set device\n","    device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","    # Make input tensor and init Swin Transformer V2, for the custom deformable version set use_deformable_block=True\n","    input = torch.rand(2, 3, 256, 256, device=device)\n","    swin_transformer: SwinTransformerV2 = swin_transformer_v2_t(in_channels=3,\n","                                                                window_size=8,\n","                                                                input_resolution=(256, 256),\n","                                                                sequential_self_attention=False,\n","                                                                use_checkpoint=False)\n","    # Model to device\n","    swin_transformer.to(device=device)\n","    # Perform forward pass\n","    features: List[torch.Tensor] = swin_transformer(input)\n","    # Print shape of features\n","    for feature in features:\n","        print(feature.shape)\n","\n","    # Update the resolution and window size of the Swin Transformer V2 and init new input\n","    swin_transformer.update_resolution(new_window_size=16, new_input_resolution=(512, 512))\n","    input = torch.rand(2, 3, 512, 512, device=device)\n","    # Perform forward pass\n","    features: List[torch.Tensor] = swin_transformer(input)\n","    # Print shape of features\n","    for feature in features:\n","        print(feature.shape)"],"metadata":{"id":"T0jmaVZzQkgj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["main()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"blGlBgByaPmE","executionInfo":{"status":"ok","timestamp":1650834725680,"user_tz":240,"elapsed":9633,"user":{"displayName":"Yilin Li","userId":"08057653249073875200"}},"outputId":"816cd2a5-4d25-4404-ed65-792aeeed63a8"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/functional.py:445: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2157.)\n","  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"]},{"output_type":"stream","name":"stdout","text":["torch.Size([2, 96, 64, 64])\n","torch.Size([2, 192, 32, 32])\n","torch.Size([2, 384, 16, 16])\n","torch.Size([2, 768, 8, 8])\n","torch.Size([2, 96, 128, 128])\n","torch.Size([2, 192, 64, 64])\n","torch.Size([2, 384, 32, 32])\n","torch.Size([2, 768, 16, 16])\n"]}]},{"cell_type":"code","source":["class ModelWrapper(object):\n","    \"\"\"\n","    This class implements a model wrapper for training a classification model.\n","    \"\"\"\n","\n","    def __init__(self,\n","                 model: Union[nn.Module, nn.DataParallel],\n","                 optimizer: torch.optim.Optimizer,\n","                 loss_function: nn.Module,\n","                 loss_function_test: nn.Module,\n","                 training_dataset: DataLoader,\n","                 test_dataset: DataLoader,\n","                 lr_schedule: Any,\n","                 augmentation: Any,\n","                 validation_metric: nn.Module,\n","                 logger: Logger,\n","                 device: str = \"cuda\") -> None:\n","        \"\"\"\n","        Constructor method\n","        :param model: (Union[nn.Module, nn.DataParallel]) Model to be trained\n","        :param optimizer: (Optimizer) Optimizer module\n","        :param loss_function: (nn.Module) Loss function\n","        :param training_dataset: (DataLoader) Training dataset\n","        :param test_dataset: (DataLoader) Test dataset\n","        :param validation_metric: (nn.Module) Validation metric\n","        :param device: (str) Device to be utilized\n","        \"\"\"\n","        # Save parameters\n","        self.model = model\n","        self.optimizer = optimizer\n","        self.loss_function = loss_function\n","        self.loss_function_test = loss_function_test\n","        self.training_dataset = training_dataset\n","        self.test_dataset = test_dataset\n","        self.lr_schedule = lr_schedule\n","        self.augmentation = augmentation\n","        self.validation_metric = validation_metric\n","        self.logger = logger\n","        self.device = device\n","        self.best_metric = 0.\n","\n","    def train(self,\n","              epochs: int = 250) -> None:\n","        \"\"\"\n","        Training function\n","        :param epoch: (int) Number of the current epoch\n","        \"\"\"\n","        # Model to device\n","        self.model.to(self.device)\n","        # Init progress bar\n","        self.progress_bar = tqdm(total=(epochs * len(self.training_dataset)))\n","        # Training loop\n","        for epoch in range(epochs):\n","            # Model into train mode\n","            self.model.train()\n","            for index, (inputs, labels) in enumerate(self.training_dataset):\n","                # Update progress bar\n","                self.progress_bar.update(n=1)\n","                # Data to device\n","                inputs = inputs.to(self.device)\n","                labels = labels.to(self.device)\n","                # Perform augmentation\n","                inputs, labels = self.augmentation(inputs, labels)\n","                # Reset gradients\n","                self.optimizer.zero_grad()\n","                # Make prediction\n","                predictions = self.model(inputs)\n","                # Calc loss\n","                loss = self.loss_function(predictions, labels)\n","                # Compute gradients\n","                loss.backward()\n","                # Perform optimization\n","                self.optimizer.step()\n","                # Print info in progress bar\n","                self.progress_bar.set_description(\n","                    \"Epoch: {} | Loss: {:.4f}\".format(epoch + 1, loss.item()))\n","                # Log loss and metric\n","                self.logger.log_metric(metric_name=\"training_loss\", value=loss.item())\n","                # Learning rate schedule step\n","                self.lr_schedule.step_update(epoch * len(self.training_dataset) + index)\n","            # Perform testing\n","            self.test(epoch=epoch)\n","            # Save metrics\n","            self.logger.save()\n","        # Close progress bar\n","        self.progress_bar.close()\n","        # Final testing\n","        print(\"Training\")\n","        self.test(train=True, print_results=True)\n","        print(\"Validation\")\n","        self.test(print_results=True)\n","\n","    @torch.no_grad()\n","    def test(self, epoch: int = -1, train: bool = False, print_results: bool = False) -> None:\n","        \"\"\"\n","        Test function\n","        :param epoch: (int) Current epoch\n","        \"\"\"\n","        # Model to device\n","        self.model.to(self.device)\n","        # Init list to store accuracies and losses\n","        metrics: List[float] = []\n","        losses: List[float] = []\n","        # Model into eval mode\n","        self.model.eval()\n","        # Training loop\n","        for index, (inputs, labels) in enumerate(self.test_dataset if not train else self.training_dataset):\n","            # Data to device\n","            inputs = inputs.to(self.device)\n","            labels = labels.to(self.device)\n","            # Make prediction\n","            predictions = self.model(inputs)\n","            # Compute loss\n","            loss = self.loss_function_test(predictions, labels)\n","            # Get metric\n","            metric = self.validation_metric(predictions, labels)\n","            # Print progress bar\n","            self.progress_bar.set_description(\n","                \"Testing | Loss: {:.4f} | Acc: {:.4f}\".format(loss.item(), metric.item()))\n","            # Save metric and loss\n","            metrics.append(metric.item())\n","            losses.append(loss.item())\n","        # Print results if utilized\n","        if print_results:\n","            print(\"Accuracy:\", np.mean(metrics))\n","            print(\"Loss:\", np.mean(losses))\n","        # Save model\n","        if not train:\n","            # Log loss and metric\n","            self.logger.log_metric(metric_name=\"test_loss\", value=np.mean(losses))\n","            self.logger.log_metric(metric_name=\"test_metric\", value=np.mean(metrics))\n","            if np.mean(metrics) > self.best_metric:\n","                # Set new best accuracy\n","                self.best_metric = np.mean(metrics)\n","                # Print info\n","                print(\"Save best model with accuracy\", self.best_metric)\n","                # Save model\n","                self.logger.save_model(\n","                    model_sate_dict={\n","                        \"model\": self.model.module.state_dict()\n","                        if isinstance(self.model, nn.DataParallel) else self.model.state_dict(),\n","                        \"acc\": self.best_metric,\n","                        \"epoch\": epoch + 1,\n","                        \"optimizer\": self.optimizer.state_dict()},\n","                    name=\"best_model\")\n","                # Save only the backbone\n","                self.logger.save_model(\n","                    model_sate_dict=self.model.module.model.state_dict()\n","                    if isinstance(self.model, nn.DataParallel) else self.model.model.state_dict(),\n","                    name=\"best_model_backbone\")"],"metadata":{"id":"I_ZvRdTJeNM4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class Accuracy(nn.Module):\n","    \"\"\"\n","    This class implements the accuracy metric.\n","    \"\"\"\n","\n","    def __init__(self) -> None:\n","        \"\"\"\n","        Constructor method\n","        \"\"\"\n","        # Call super constructor\n","        super(Accuracy, self).__init__()\n","\n","    def forward(self, prediction: torch.Tensor, label: torch.Tensor) -> torch.Tensor:\n","        \"\"\"\n","        Forward pass computes the accuracy metric\n","        :param prediction: (torch.Tensor) Prediction of the shape [batch size, classes] (one-hot)\n","        :param label: (torch.Tensor) Classification label of the shape [batch size]\n","        :return: (torch.Tensor) Accuracy metric\n","        \"\"\"\n","        # Threshold prediction with arg max\n","        prediction = prediction.argmax(dim=-1)\n","        # Compute accuracy\n","        accuracy = (prediction == label).sum() / float(prediction.shape[0])\n","        return accuracy"],"metadata":{"id":"jALERWGieSue"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["_, term_width = os.popen('stty size', 'r').read().split()\n","term_width = int(term_width)\n","\n","TOTAL_BAR_LENGTH = 30.\n","last_time = time.time()\n","begin_time = last_time\n","\n","\n","def progress_bar(current, total, msg=None):\n","    global last_time, begin_time\n","    if current == 0:\n","        begin_time = time.time()  # Reset for new bar.\n","\n","    cur_len = int(TOTAL_BAR_LENGTH * current / total)\n","    rest_len = int(TOTAL_BAR_LENGTH - cur_len) - 1\n","\n","    sys.stdout.write(' [')\n","    for i in range(cur_len):\n","        sys.stdout.write('=')\n","    sys.stdout.write('>')\n","    for i in range(rest_len):\n","        sys.stdout.write('.')\n","    sys.stdout.write(']')\n","\n","    cur_time = time.time()\n","    step_time = cur_time - last_time\n","    last_time = cur_time\n","    tot_time = cur_time - begin_time\n","\n","    L = []\n","    L.append('  Step: %s' % format_time(step_time))\n","    L.append(' | Tot: %s' % format_time(tot_time))\n","    if msg:\n","        L.append(' | ' + msg)\n","\n","    msg = ''.join(L)\n","    sys.stdout.write(msg)\n","    for i in range(term_width - int(TOTAL_BAR_LENGTH) - len(msg) - 3):\n","        sys.stdout.write(' ')\n","\n","    # Go back to the center of the bar.\n","    for i in range(term_width - int(TOTAL_BAR_LENGTH / 2) + 2):\n","        sys.stdout.write('\\b')\n","    sys.stdout.write(' %d/%d ' % (current + 1, total))\n","\n","    if current < total - 1:\n","        sys.stdout.write('\\r')\n","    else:\n","        sys.stdout.write('\\n')\n","    sys.stdout.flush()\n","\n","\n","def format_time(seconds):\n","    days = int(seconds / 3600 / 24)\n","    seconds = seconds - days * 3600 * 24\n","    hours = int(seconds / 3600)\n","    seconds = seconds - hours * 3600\n","    minutes = int(seconds / 60)\n","    seconds = seconds - minutes * 60\n","    secondsf = int(seconds)\n","    seconds = seconds - secondsf\n","    millis = int(seconds * 1000)\n","\n","    f = ''\n","    i = 1\n","    if days > 0:\n","        f += str(days) + 'D'\n","        i += 1\n","    if hours > 0 and i <= 2:\n","        f += str(hours) + 'h'\n","        i += 1\n","    if minutes > 0 and i <= 2:\n","        f += str(minutes) + 'm'\n","        i += 1\n","    if secondsf > 0 and i <= 2:\n","        f += str(secondsf) + 's'\n","        i += 1\n","    if millis > 0 and i <= 2:\n","        f += str(millis) + 'ms'\n","        i += 1\n","    if f == '':\n","        f = '0ms'\n","    return f\n","\n","\n","class ClassificationModelWrapper(nn.Module):\n","    \"\"\"\n","    Wraps a Swin Transformer V2 model to perform image classification.\n","    \"\"\"\n","\n","    def __init__(self, model: SwinTransformerV2, number_of_classes: int = 10, output_channels: int = 768) -> None:\n","        \"\"\"\n","        Constructor method\n","        :param model: (SwinTransformerV2) Swin Transformer V2 model\n","        :param number_of_classes: (int) Number of classes to predict\n","        :param output_channels: (int) Output channels of the last feature map of the Swin Transformer V2 model\n","        \"\"\"\n","        # Call super constructor\n","        super(ClassificationModelWrapper, self).__init__()\n","        # Save model\n","        self.model: SwinTransformerV2 = model\n","        # Init adaptive average pooling layer\n","        self.pooling: nn.Module = nn.AdaptiveAvgPool2d(1)\n","        # Init classification head\n","        self.classification_head: nn.Module = nn.Linear(in_features=output_channels, out_features=number_of_classes)\n","\n","    def forward(self, input: torch.Tensor) -> torch.Tensor:\n","        \"\"\"\n","        Forward pass\n","        :param input: (torch.Tensor) Input tensor of the shape [batch size, channels, height, width]\n","        :return: (torch.Tensor) Output classification of the shape [batch size, number of classes]\n","        \"\"\"\n","        # Compute features\n","        features: List[torch.Tensor] = self.model(input)\n","        # Compute classification\n","        classification: torch.Tensor = self.classification_head(self.pooling(features[-1]).flatten(start_dim=1))\n","        return classification"],"metadata":{"id":"3XtFsUoOeToH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def train(args) -> None:\n","    if args.dataset == \"cifar10\":\n","        print(\"CIFAR10 dataset utilized\")\n","        # Init transformations\n","        transform_train = transforms.Compose([\n","            rand_augment_transform(config_str=\"rand-m9-n3-mstd0.5\", hparams={\"img_mean\": (125, 123, 114)}),\n","            transforms.ToTensor(),\n","            transforms.Normalize(mean=(0.4914, 0.4822, 0.4465), std=(0.2023, 0.1994, 0.2010))\n","        ])\n","        transform_test = transforms.Compose([\n","            transforms.ToTensor(),\n","            transforms.Normalize(mean=(0.4914, 0.4822, 0.4465), std=(0.2023, 0.1994, 0.2010))\n","        ])\n","        # Init datasets\n","        training_dataset = torchvision.datasets.CIFAR10(root=\"./CIFAR10\", train=True, download=True,\n","                                                        transform=transform_train)\n","        training_dataset = DataLoader(training_dataset, batch_size=args.batch_size, shuffle=True,\n","                                      num_workers=min(40, args.batch_size), pin_memory=True, prefetch_factor=10)\n","        test_dataset = torchvision.datasets.CIFAR10(root=\"./CIFAR10\", train=False, download=True,\n","                                                    transform=transform_test)\n","        test_dataset = DataLoader(test_dataset, batch_size=args.batch_size, shuffle=False,\n","                                  num_workers=min(40, args.batch_size), pin_memory=True, prefetch_factor=10)\n","    else:\n","        print(\"Places365 dataset utilized\")\n","        # Init transformations\n","        transform_train = transforms.Compose([\n","            rand_augment_transform(config_str=\"rand-m9-n3-mstd0.5\", hparams={\"img_mean\": (124, 116, 104)}),\n","            transforms.ToTensor(),\n","            transforms.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n","        ])\n","        transform_test = transforms.Compose([\n","            transforms.ToTensor(),\n","            transforms.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n","        ])\n","        # Init datasets\n","        training_dataset = torchvision.datasets.ImageFolder(root=os.path.join(args.dataset_path, \"train\"),\n","                                                            transform=transform_train)\n","        training_dataset = DataLoader(training_dataset, batch_size=args.batch_size, shuffle=True,\n","                                      num_workers=min(20, args.batch_size), pin_memory=True, prefetch_factor=10)\n","        test_dataset = torchvision.datasets.ImageFolder(root=os.path.join(args.dataset_path, \"val\"),\n","                                                        transform=transform_test)\n","        test_dataset = DataLoader(test_dataset, batch_size=args.batch_size, shuffle=False,\n","                                  num_workers=min(20, args.batch_size), pin_memory=True, prefetch_factor=10)\n","    # Init model\n","    if args.model_type == \"t\":\n","        model_function = swin_transformer_v2_t\n","        output_channels = 768\n","    elif args.model_type == \"s\":\n","        model_function = swin_transformer_v2_s\n","        output_channels = 768\n","    elif args.model_type == \"b\":\n","        model_function = swin_transformer_v2_b\n","        output_channels = 1024\n","    elif args.model_type == \"l\":\n","        model_function = swin_transformer_v2_l\n","        output_channels = 1536\n","    elif args.model_type == \"h\":\n","        model_function = swin_transformer_v2_h\n","        output_channels = 2816\n","    else:\n","        model_function = swin_transformer_v2_g\n","        output_channels = 4096\n","    model = ClassificationModelWrapper(\n","        model=model_function(input_resolution=(32, 32) if args.dataset == \"cifar10\" else (256, 256),\n","                             window_size=8, dropout_path=0.1, use_deformable_block=args.deformable),\n","        number_of_classes=10 if args.dataset == \"cifar10\" else 365,\n","        output_channels=output_channels)\n","    # Print number of parameters\n","    print(\"# parameters\", sum([p.numel() for p in model.parameters()]))\n","    # Model to device\n","    model.to(args.device)\n","    # Init data parallel\n","    if args.data_parallel:\n","        model = nn.DataParallel(model)\n","    # Init optimizer\n","    optimizer = torch.optim.AdamW(model.parameters(), lr=args.lr, weight_decay=args.weight_decay, betas=(0.9, 0.95))\n","    # Init learning rate schedule\n","    lr_schedule = CosineLRScheduler(optimizer=optimizer,\n","                                    t_initial=args.epochs * len(training_dataset),\n","                                    t_mul=1., lr_min=5e-6, warmup_lr_init=5e-7,\n","                                    warmup_t=(10 if args.dataset == \"cifar10\" else 2) * len(training_dataset),\n","                                    cycle_limit=1,\n","                                    t_in_epochs=False)\n","    # Init loss function\n","    loss_function = SoftTargetCrossEntropy()\n","    # Init model wrapper\n","    model_wrapper = ModelWrapper(model=model,\n","                                 optimizer=optimizer,\n","                                 loss_function=loss_function,\n","                                 loss_function_test=nn.CrossEntropyLoss(),\n","                                 training_dataset=training_dataset,\n","                                 test_dataset=test_dataset,\n","                                 lr_schedule=lr_schedule,\n","                                 augmentation=Mixup(mixup_alpha=1.0,\n","                                                    cutmix_alpha=1.0,\n","                                                    num_classes=10 if args.dataset == \"cifar10\" else 365,\n","                                                    label_smoothing=0.1),\n","                                 validation_metric=Accuracy(),\n","                                 logger=Logger(experiment_path_extension=\"_Swin_{}{}_{}\".format(\n","                                     args.model_type,\n","                                     \"_deformable\" if args.deformable else \"\",\n","                                     args.dataset)),\n","                                 device=args.device)\n","    # Perform training\n","    model_wrapper.train(epochs=args.epochs)"],"metadata":{"id":"e2YjEttBeeqn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train()"],"metadata":{"id":"9pF5FvX6ef-P"},"execution_count":null,"outputs":[]}]}