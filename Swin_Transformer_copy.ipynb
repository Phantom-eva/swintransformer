{"cells":[{"cell_type":"markdown","metadata":{"id":"nm1WssSU3P3Q"},"source":["EECS 542 Final Project - Swin Transformer\n","\n","Jiangyan Feng, Pengfei Gao, Yilin Li, Zekun Li"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":12321,"status":"ok","timestamp":1650927966219,"user":{"displayName":"Jiangyan Feng","userId":"00140510051904189650"},"user_tz":240},"id":"jT8AWQVU0nB9","outputId":"1bb8e0e1-e173-4250-bf4b-413d9f7f7d05"},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting timm\n","  Downloading timm-0.5.4-py3-none-any.whl (431 kB)\n","\u001b[?25l\r\u001b[K     |▊                               | 10 kB 27.1 MB/s eta 0:00:01\r\u001b[K     |█▌                              | 20 kB 34.1 MB/s eta 0:00:01\r\u001b[K     |██▎                             | 30 kB 17.8 MB/s eta 0:00:01\r\u001b[K     |███                             | 40 kB 13.3 MB/s eta 0:00:01\r\u001b[K     |███▉                            | 51 kB 11.7 MB/s eta 0:00:01\r\u001b[K     |████▋                           | 61 kB 13.5 MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 71 kB 12.6 MB/s eta 0:00:01\r\u001b[K     |██████                          | 81 kB 12.8 MB/s eta 0:00:01\r\u001b[K     |██████▉                         | 92 kB 13.9 MB/s eta 0:00:01\r\u001b[K     |███████▋                        | 102 kB 15.0 MB/s eta 0:00:01\r\u001b[K     |████████▍                       | 112 kB 15.0 MB/s eta 0:00:01\r\u001b[K     |█████████▏                      | 122 kB 15.0 MB/s eta 0:00:01\r\u001b[K     |█████████▉                      | 133 kB 15.0 MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 143 kB 15.0 MB/s eta 0:00:01\r\u001b[K     |███████████▍                    | 153 kB 15.0 MB/s eta 0:00:01\r\u001b[K     |████████████▏                   | 163 kB 15.0 MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 174 kB 15.0 MB/s eta 0:00:01\r\u001b[K     |█████████████▊                  | 184 kB 15.0 MB/s eta 0:00:01\r\u001b[K     |██████████████▍                 | 194 kB 15.0 MB/s eta 0:00:01\r\u001b[K     |███████████████▏                | 204 kB 15.0 MB/s eta 0:00:01\r\u001b[K     |████████████████                | 215 kB 15.0 MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 225 kB 15.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████▌              | 235 kB 15.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████▎             | 245 kB 15.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 256 kB 15.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████▊            | 266 kB 15.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████▌           | 276 kB 15.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▎          | 286 kB 15.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 296 kB 15.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▉         | 307 kB 15.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▌        | 317 kB 15.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 327 kB 15.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 337 kB 15.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▉      | 348 kB 15.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▋     | 358 kB 15.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▍    | 368 kB 15.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 378 kB 15.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▉   | 389 kB 15.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▋  | 399 kB 15.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▍ | 409 kB 15.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 419 kB 15.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 430 kB 15.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 431 kB 15.0 MB/s \n","\u001b[?25hRequirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from timm) (0.12.0+cu113)\n","Requirement already satisfied: torch\u003e=1.4 in /usr/local/lib/python3.7/dist-packages (from timm) (1.11.0+cu113)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch\u003e=1.4-\u003etimm) (4.2.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torchvision-\u003etimm) (2.23.0)\n","Requirement already satisfied: pillow!=8.3.*,\u003e=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision-\u003etimm) (7.1.2)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchvision-\u003etimm) (1.21.6)\n","Requirement already satisfied: idna\u003c3,\u003e=2.5 in /usr/local/lib/python3.7/dist-packages (from requests-\u003etorchvision-\u003etimm) (2.10)\n","Requirement already satisfied: chardet\u003c4,\u003e=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests-\u003etorchvision-\u003etimm) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,\u003c1.26,\u003e=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests-\u003etorchvision-\u003etimm) (1.24.3)\n","Requirement already satisfied: certifi\u003e=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests-\u003etorchvision-\u003etimm) (2021.10.8)\n","Installing collected packages: timm\n","Successfully installed timm-0.5.4\n","PyTorch Version:  1.11.0+cu113\n","Torchvision Version:  0.12.0+cu113\n","cuda:0\n"]}],"source":["!pip install timm\n","import os\n","import sys\n","import numpy as np\n","import math\n","import random\n","import cv2\n","import copy\n","from PIL import Image\n","# import einops\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.optim import lr_scheduler, AdamW\n","from torch.utils.data import Dataset\n","import torchvision\n","from torchvision import datasets, transforms\n","from torchvision.transforms import InterpolationMode\n","from timm.models.layers import to_2tuple\n","from timm.scheduler.cosine_lr import CosineLRScheduler\n","from timm.scheduler.step_lr import StepLRScheduler\n","from timm.loss import SoftTargetCrossEntropy\n","from timm.data import Mixup\n","from timm.data import create_transform\n","\n","print(\"PyTorch Version: \", torch.__version__)\n","print(\"Torchvision Version: \", torchvision.__version__)\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","print(device)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":46538,"status":"ok","timestamp":1650928012745,"user":{"displayName":"Jiangyan Feng","userId":"00140510051904189650"},"user_tz":240},"id":"uf7_Y_5smCb2","outputId":"56979662-8f9b-4063-bddf-74e0fe1bc588"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"markdown","metadata":{"id":"s6dpcd8S3DK8"},"source":["1. Patch partition, Linear embedding, Patch merging (Zekun Li)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Y7E4CKQp26pt"},"outputs":[],"source":["class PatchEmbed(nn.Module):\n","  def __init__(self, patch_size=4, in_c=3, output_dim=96, norm=None):\n","    super(PatchEmbed, self).__init__()\n","    self.patch_size = patch_size\n","    self.linear = nn.Conv2d(in_c, output_dim, kernel_size=self.patch_size, stride=self.patch_size, padding=0) # C -\u003e output_dim, or use in_chans\n","    self.norm = nn.LayerNorm(output_dim)\n","\n","  def forward(self, x): ## x: B * C * H * W\n","    B, C, H, W = x.shape\n","    if (H % self.patch_size) or (W % self.patch_size):\n","      x = F.pad(x, (0, self.patch_size - W % self.patch_size, 0, self.patch_size - H % self.patch_size, 0, 0))\n","    x = self.linear(x)\n","    x = x.flatten(2)\n","    x = x.transpose(1, 2)\n","    if self.norm: x = self.norm(x)\n","    return x \n","\n","class PatchMerging(nn.Module):\n","  def __init__(self, in_c, norm=nn.LayerNorm):\n","    super(PatchMerging, self).__init__()\n","    # self.norm = norm\n","    self.norm = nn.LayerNorm(in_c * 4)\n","    self.linear = nn.Linear(in_c * 4, in_c * 2, bias=False)\n","\n","  def forward(self, x, H, W): ## x: B * L * C\n","    B, _, C = x.shape\n","    x = x.view(B, H, W, C)\n","    if H % 2 or W % 2:\n","      x = F.pad(x, (0, 0, 0, W % 2, 0, H % 2))\n","    x = torch.cat([x[:, 0::2, 0::2, :], x[:, 1::2, 0::2, :],\n","                   x[:, 0::2, 1::2, :], x[:, 1::2, 1::2, :]], -1).view(B, -1, C * 4)\n","    # norm = nn.LayerNorm(C * 4)\n","    # linear = nn.Linear(C * 4, C * 2, bias=False)\n","    return self.linear(self.norm(x))  ## x: B * HW/4 * 2C"]},{"cell_type":"markdown","metadata":{"id":"hAg9jOfi3JhV"},"source":["2. Swin transformer block basic function (Yilin Li)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CwX4_ZeV3OTi"},"outputs":[],"source":["class Mlp(nn.Module):\n","    \"\"\"\n","    a 2-layer MLP with GELU nonlinearity activation function\n","\n","    Args:\n","        in_features: input dimension\n","        hidden_features: hidden layer dimension\n","        out_features: output dimension\n","        act_layer: type of activation function(here we use GELU nonlinearity)\n","        drop: (dropout layer parameter) probability of an element to be zeroed\n","    \"\"\"\n","    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0.):\n","        super().__init__()\n","        out_features = out_features or in_features\n","        hidden_features = hidden_features or in_features\n","        self.fc1 = nn.Linear(in_features, hidden_features)\n","        self.act = act_layer()\n","        self.fc2 = nn.Linear(hidden_features, out_features)\n","        self.drop = nn.Dropout(drop)\n","\n","    def forward(self, x):\n","        \"\"\"\n","        Args:\n","            x: input features\n","        \"\"\"\n","        x = self.fc1(x)\n","        x = self.act(x)\n","        x = self.drop(x)\n","        x = self.fc2(x)\n","        x = self.drop(x)\n","        return x\n","\n","def window_partition(x, window_size):\n","    \"\"\"\n","    Args:\n","        x: (B, H, W, C)\n","        window_size (int): shape of window is (window_size * window_size)\n","\n","    Returns:\n","        windows: (num_windows*B, window_size, window_size, C)\n","    \"\"\"\n","    B, H, W, C = x.shape\n","    patch_num_y = H // window_size\n","    patch_num_x = W // window_size\n","    x = torch.reshape(x, (-1, patch_num_y, window_size, patch_num_x, window_size, C))\n","    x = x.permute(0, 1, 3, 2, 4, 5)\n","    windows = torch.reshape(x, (-1, window_size, window_size, C))\n","    return windows\n","\n","def window_reverse(windows, window_size, H, W):\n","    \"\"\"\n","    Args:\n","        windows: (num_windows*B, window_size, window_size, C)\n","        window_size (int): shape of window is (window_size * window_size)\n","        H (int): height of image\n","        W (int): width of image\n","\n","    Returns:\n","        x: (B, H, W, C)\n","    \"\"\"\n","    patch_num_y = H // window_size\n","    patch_num_x = W // window_size\n","    _, _, _, C = windows.shape\n","    x = torch.reshape(windows, (-1, patch_num_y, patch_num_x, window_size, window_size, C))\n","    x = x.permute(0, 1, 3, 2, 4, 5)\n","    x = torch.reshape(x, (-1, H, W, C))\n","    return x\n","\n","class DropPath(nn.Module):\n","    \"\"\"\n","    Drop paths (Stochastic Depth) per sample  (when applied in main path of residual blocks).\n","\n","    Args:\n","        drop_prob (float): probability of drop \n","        scale_by_keep (bool): \n","    \"\"\"\n","    def __init__(self, drop_prob=None, scale_by_keep=True):\n","        super().__init__()\n","        self.drop_prob = drop_prob\n","        self.scale_by_keep = scale_by_keep\n","\n","    def forward(self, x):\n","        \"\"\"\n","        Args:\n","            x: input\n","        \"\"\"\n","        if self.drop_prob == 0. or not self.training:\n","            return x\n","        keep_prob = 1 - self.drop_prob\n","        shape = (x.shape[0],) + (1,) * (x.ndim - 1)\n","        random_tensor = x.new_empty(shape).bernoulli_(keep_prob)\n","        if keep_prob \u003e 0.0 and self.scale_by_keep:\n","            random_tensor.div_(keep_prob)\n","        return x * random_tensor\n","\n","def relative_position(window_size):\n","    \"\"\"\n","    get pair-wise relative position index for each token inside the window\n","\n","    Args:\n","    window_size (tuple[int]): The height and width of the window.\n","    \n","    Returns:\n","    relative_position_index (Wh*Ww, Wh*Ww)\n","    \"\"\"\n","    coords_h = torch.arange(window_size[0])\n","    coords_w = torch.arange(window_size[1])\n","    coords = torch.stack(torch.meshgrid([coords_h, coords_w]))  \n","    coords_flatten = torch.flatten(coords, 1)  \n","    relative_coords = coords_flatten[:, :, None] - coords_flatten[:, None, :]  \n","    relative_coords = relative_coords.permute(1, 2, 0).contiguous()  \n","    relative_coords[:, :, 0] += window_size[0] - 1  \n","    relative_coords[:, :, 1] += window_size[1] - 1\n","    relative_coords[:, :, 0] *= 2 * window_size[1] - 1\n","    relative_position_index = relative_coords.sum(-1)  \n","    return relative_position_index\n","\n","def trunc_normal(tensor, mean=0., std=1., a=-2., b=2.):\n","    def norm_cdf(x):\n","        # Computes standard normal cumulative distribution function\n","        return (1. + math.erf(x / math.sqrt(2.))) / 2.\n","\n","    with torch.no_grad():\n","        # Values are generated by using a truncated uniform distribution and\n","        # then using the inverse CDF for the normal distribution.\n","        # Get upper and lower cdf values\n","        l = norm_cdf((a - mean) / std)\n","        u = norm_cdf((b - mean) / std)\n","        # Uniformly fill tensor with values from [l, u], then translate to\n","        # [2l-1, 2u-1].\n","        tensor.uniform_(2 * l - 1, 2 * u - 1)\n","        # Use inverse cdf transform for normal distribution to get truncated\n","        # standard normal\n","        tensor.erfinv_()\n","        # Transform to proper mean, std\n","        tensor.mul_(std * math.sqrt(2.))\n","        tensor.add_(mean)\n","        # Clamp to ensure it's in the proper range\n","        tensor.clamp_(min=a, max=b)\n","        return tensor\n","\n","class WindowAttention(nn.Module):\n","    \"\"\" W-MSA / SW-MSA\n","\n","    Args:\n","        dim (int): Number of input channels.\n","        window_size (tuple[int]): The height and width of the window.\n","        num_heads (int): Number of attention heads.\n","        qkv_bias (bool, optional):  If True, add a learnable bias to query, key, value. Default: True\n","        qk_scale (float | None, optional): Override default qk scale of head_dim ** -0.5 if set\n","        attn_drop (float, optional): Dropout ratio of attention weight. Default: 0.0\n","        proj_drop (float, optional): Dropout ratio of output. Default: 0.0\n","    \"\"\"\n","\n","    def __init__(self, dim, window_size, num_heads, qkv_bias=True, qk_scale=None, attn_drop=0., proj_drop=0.):\n","\n","        super().__init__()\n","        self.dim = dim\n","        self.window_size = window_size  \n","        self.num_heads = num_heads\n","        self.scale = qk_scale or (dim // num_heads) ** (-0.5)\n","\n","        # define a parameter table of relative position bias\n","        self.relative_position_bias_table = nn.Parameter(\n","            torch.zeros((2 * window_size[0] - 1) * (2 * window_size[1] - 1), num_heads)) \n","\n","        self.register_buffer(\"relative_position_index\", relative_position(self.window_size))\n","\n","        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n","        self.attn_drop = nn.Dropout(attn_drop)\n","        self.proj = nn.Linear(dim, dim)\n","        self.proj_drop = nn.Dropout(proj_drop)\n","\n","        trunc_normal(self.relative_position_bias_table, std=.02)\n","        \n","        self.softmax = nn.Softmax(dim=-1)\n","\n","    def forward(self, x, mask=None):\n","        \"\"\"\n","        Args:\n","            x: input features with shape of (num_windows*B, N, C)\n","            mask: (0/-inf) mask with shape of (num_windows, Wh*Ww, Wh*Ww) or None\n","        \"\"\"\n","        B_, N, C = x.shape\n","        qkv = self.qkv(x).reshape(B_, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n","        q, k, v = qkv[0], qkv[1], qkv[2]  # make torchscript happy (cannot use tensor as tuple)\n","\n","        q = q * self.scale\n","        attn = (q @ k.transpose(-2, -1))\n","\n","        relative_position_bias = self.relative_position_bias_table[self.relative_position_index.view(-1)].view(\n","            self.window_size[0] * self.window_size[1], self.window_size[0] * self.window_size[1], -1)  \n","        relative_position_bias = relative_position_bias.permute(2, 0, 1).contiguous()  \n","        attn = attn + relative_position_bias.unsqueeze(0)\n","\n","        if mask is not None:\n","            nW = mask.shape[0]\n","            attn = attn.view(B_ // nW, nW, self.num_heads, N, N) + mask.unsqueeze(1).unsqueeze(0)\n","            attn = attn.view(-1, self.num_heads, N, N)\n","            attn = self.softmax(attn)\n","        else:\n","            attn = self.softmax(attn)\n","\n","        attn = self.attn_drop(attn)\n","\n","        x = (attn @ v).transpose(1, 2).reshape(B_, N, C)\n","        x = self.proj(x)\n","        x = self.proj_drop(x)\n","        return x"]},{"cell_type":"markdown","metadata":{"id":"LIa3-fFp3lXw"},"source":["3. Model construction (Pengfei Gao)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oCTZ_che3wCL"},"outputs":[],"source":["def get_mask(input_resolution, window_size, shift_size):\n","    img_mask = torch.zeros((1, input_resolution[0], input_resolution[1], 1))\n","    h_slices = (slice(0, -window_size), slice(-window_size, -shift_size), slice(-shift_size, None))\n","    w_slices = (slice(0, -window_size), slice(-window_size, -shift_size), slice(-shift_size, None))\n","    count = 0\n","    for h in h_slices:\n","        for w in w_slices:\n","            img_mask[:, h, w, :] = count\n","            count += 1\n","    windows = window_partition(img_mask, window_size).view(-1, window_size ** 2)\n","    mask = windows.unsqueeze(1) - windows.unsqueeze(2)\n","    mask = mask.masked_fill(mask != 0, float(-100.00)).masked_fill(mask == 0, float(0.00))\n","    return mask\n","\n","def normal_init(m):\n","    if isinstance(m, nn.Linear):\n","        trunc_normal(m.weight, std=0.02)\n","        if isinstance(m, nn.Linear) and m.bias is not None:\n","            nn.init.constant_(m.bias, 0)\n","    elif isinstance(m, nn.LayerNorm):\n","       nn.init.constant_(m.bias, 0)\n","       nn.init.constant_(m.weight, 1.0)\n","    \n","\n","class SwinTransformerBlock(nn.Module):\n","    def __init__(self, dim, input_resolution, num_heads, window_size=7, shift_size=0,\n","                  mlp_ratio=4., qkv_bias=True, qk_scale=None, drop=0., attn_drop=0., drop_path=0.):\n","        super(SwinTransformerBlock, self).__init__()\n","        self.dim = dim\n","        self.input_resolution = input_resolution\n","        print(input_resolution)\n","        self.window_size = window_size\n","        # self.shift_size = shift_size\n","        self.shift_size = window_size // 2\n","        self.attention_mask = get_mask(self.input_resolution, self.window_size, self.shift_size).to(device)\n","        self.num_heads = num_heads\n","        #LayerNorm\n","        self.LN1 = nn.LayerNorm(dim)\n","        self.LN2 = nn.LayerNorm(dim)\n","        self.LN3 = nn.LayerNorm(dim)\n","        self.LN4 = nn.LayerNorm(dim)\n","        #MLP\n","        self.MLP1 = Mlp(in_features=self.dim, hidden_features=int(mlp_ratio*self.dim), drop=drop)\n","        self.MLP2 = Mlp(in_features=self.dim, hidden_features=int(mlp_ratio*self.dim), drop=drop)\n","        #WindowAttention\n","        self.WindowAttention1 = WindowAttention(dim, window_size=to_2tuple(self.window_size), num_heads=num_heads,\n","                qkv_bias=qkv_bias, qk_scale=qk_scale, attn_drop=attn_drop, proj_drop=drop)\n","        self.WindowAttention2 = WindowAttention(dim, window_size=to_2tuple(self.window_size), num_heads=num_heads,\n","                qkv_bias=qkv_bias, qk_scale=qk_scale, attn_drop=attn_drop, proj_drop=drop)\n","        #Drop Path\n","        self.drop_path1 = DropPath(drop_path[0] if drop_path[0] \u003e 0. else 0.)\n","        self.drop_path2 = DropPath(drop_path[1] if drop_path[1] \u003e 0. else 0.)\n","\n","    def forward(self,x):\n","        H, W = self.input_resolution\n","        #first block\n","        B, L, C = x.shape\n","\n","        initial_x = x\n","        x = self.LN1(x).reshape(B,H,W,C)\n","        # partition windows\n","        # print(\"x\",x.shape)\n","        x_windows = window_partition(x, self.window_size)\n","        x_windows = x_windows.reshape(-1, self.window_size ** 2, C)\n","        # W-MSA\n","        WindowAttention1 = self.WindowAttention1(x_windows, mask=None)\n","        # merge windows\n","        WindowAttention1 = WindowAttention1.reshape(-1, self.window_size, self.window_size, C)\n","        x = window_reverse(WindowAttention1, self.window_size, H, W)\n","        x = x.reshape(B, H * W, C)\n","        #FFN\n","        x = initial_x + self.drop_path1(x)\n","        initial_x = x\n","        x = self.LN2(x)\n","        x = self.MLP1(x)\n","        x = self.drop_path1(x)\n","        x += initial_x\n","\n","        #second block\n","        B, L, C = x.shape\n","\n","        # x_initial = x\n","        initial_x = x\n","        x = self.LN3(x).reshape(B,H,W,C)\n","        #shift\n","        x = torch.roll(x, shifts=(-self.shift_size, -self.shift_size), dims=(1, 2))\n","        # partition windows\n","        x_windows = window_partition(x, self.window_size)\n","        x_windows = x_windows.reshape(-1, self.window_size ** 2, C)\n","        # SW-MSA\n","        WindowAttention2 = self.WindowAttention2(x_windows, mask=self.attention_mask)\n","        # merge windows\n","        WindowAttention2 = WindowAttention2.reshape(-1, self.window_size, self.window_size, C)\n","        x = window_reverse(WindowAttention2, self.window_size, H, W)\n","        x = torch.roll(x, shifts=(self.shift_size, self.shift_size), dims=(1, 2))\n","        x = x.reshape(B, H * W, C)\n","        #FFN\n","        x = initial_x + self.drop_path2(x)\n","        initial_x = x\n","        x = self.LN4(x)\n","        x = self.MLP2(x)\n","        x = self.drop_path2(x)\n","        x += initial_x\n","        \n","        return x\n","\n","\n","class SwinTransformerModel(nn.Module):\n","    def __init__(self, img_size=224, patch_size=4, in_chans=3, num_classes=1000,\n","                  embed_dim=96, depths=[2, 2, 6, 2], num_heads=[3, 6, 12, 24],\n","                  window_size=7, mlp_ratio=4., qkv_bias=True, qk_scale=None,\n","                  drop_rate=0., attn_drop_rate=0., drop_path_rate=0.1,\n","                  norm_layer=nn.LayerNorm, ape=False, patch_norm=True,\n","                  use_checkpoint=False, **kwargs):\n","        super(SwinTransformerModel, self).__init__()\n","        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, sum(depths))]  # stochastic depth decay rule\n","        self.img_size = img_size\n","        self.patch_size = patch_size\n","        self.patches_resolution = [self.img_size // self.patch_size, self.img_size // self.patch_size]\n","        self.embed_dim = embed_dim\n","        self.patch_embed = PatchEmbed(patch_size=self.patch_size, in_c=in_chans, output_dim=self.embed_dim)\n","        self.num_features = int(self.embed_dim * 2 ** (self.patch_size - 1))\n","        self.input_resolution = (self.patches_resolution[0], self.patches_resolution[1])\n","        self.block1 = SwinTransformerBlock(dim = int(self.embed_dim), \n","                                           input_resolution=(self.patches_resolution[0], self.patches_resolution[1]), \n","                                           num_heads=num_heads[0],\n","                                           drop_path = dpr[sum(depths[:0]):sum(depths[:0 + 1])])\n","\n","        self.block2 = SwinTransformerBlock(dim = int(self.embed_dim * 2), input_resolution=(self.patches_resolution[0] // 2, self.patches_resolution[1] // 2), num_heads=num_heads[1],drop_path = dpr[sum(depths[:1]):sum(depths[:1 + 1])])\n","        # self.block3 = SwinTransformerBlock(dim = int(self.embed_dim * 4), input_resolution=(self.patches_resolution[0] // 4, self.patches_resolution[1] // 4), num_heads=num_heads[2])\n","        self.block3 = nn.ModuleList([\n","                                     SwinTransformerBlock(dim = int(self.embed_dim * 4), \n","                                                          input_resolution=(self.patches_resolution[0] // 4, self.patches_resolution[1] // 4), \n","                                                          num_heads=num_heads[2],\n","                                                          drop_path = dpr[(sum(depths[:2])+i*2):(sum(depths[:2])+(i+1)*2)])\n","                                     for i in range(3)])\n","        self.block4 = SwinTransformerBlock(dim = int(self.embed_dim * 8), \n","                                           input_resolution=(self.patches_resolution[0] // 8, self.patches_resolution[1] // 8), \n","                                           num_heads=num_heads[3],\n","                                           drop_path = dpr[sum(depths[:3]):sum(depths[:3 + 1])])\n","        self.patch_merging1 = PatchMerging(int(self.embed_dim))\n","        self.patch_merging2 = PatchMerging(int(self.embed_dim * 2))\n","        self.patch_merging3 = PatchMerging(int(self.embed_dim * 4))\n","        self.final_layer = nn.Linear(int(self.embed_dim * 8), num_classes)\n","        self.norm = nn.LayerNorm(self.num_features)\n","        self.avgpool = nn.AdaptiveAvgPool1d(1)\n","        self.head = nn.Linear(self.num_features, num_classes) if num_classes \u003e 0 else nn.Identity()\n","        \n","        self.apply(self.weight_init)\n","        \n","    def weight_init(self, m):\n","        normal_init(m)\n","\n","    def forward(self, image):\n","        patch_embed = self.patch_embed(image)\n","        #print(\"embed done.\")\n","        x = self.block1(patch_embed)\n","        #print(\"block1 done.\")\n","        x = self.patch_merging1(x,self.input_resolution[0],self.input_resolution[1])\n","        #print(\"patch merging1 done.\")\n","        x = self.block2(x)\n","        #print(\"block2 done.\")\n","        x = self.patch_merging2(x,self.input_resolution[0]//2,self.input_resolution[1]//2)\n","        #print(\"merging2 done.\")\n","        for blk in self.block3:\n","            x = blk(x)\n","\n","        x = self.patch_merging3(x,self.input_resolution[0]//4,self.input_resolution[1]//4)\n","        x = self.block4(x)\n","\n","        x = self.norm(x)\n","        x = self.avgpool(x.transpose(1, 2))\n","        x = torch.flatten(x, 1)\n","        x = self.head(x)\n","        return x\n"]},{"cell_type":"markdown","metadata":{"id":"5sLATHiD3w_k"},"source":["4. Data processing and training (Jiangyan Feng)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"534-4phTldfw"},"outputs":[],"source":["# logger\n","import logging\n","import time\n","\n","def log_creater(output_dir):\n","    if not os.path.exists(output_dir):\n","        os.makedirs(output_dir)\n","    logger_name = '{}.log'.format(time.strftime('%Y-%m-%d-%H-%M'))\n","    final_log_file = os.path.join(output_dir,logger_name)\n","    # creat a log\n","    logger = logging.getLogger('train_log')\n","    logger.setLevel(logging.DEBUG)\n","    if not logger.handlers:\n","        # FileHandler\n","        file = logging.FileHandler(final_log_file)\n","        file.setLevel(logging.DEBUG)\n","        # StreamHandler\n","        stream = logging.StreamHandler()\n","        stream.setLevel(logging.DEBUG)\n","        # Formatter\n","        formatter = logging.Formatter('[%(asctime)s][line: %(lineno)d] ==\u003e %(message)s')\n","        # setFormatter\n","        file.setFormatter(formatter)\n","        stream.setFormatter(formatter)\n","        # addHandler\n","        logger.addHandler(file)\n","        logger.addHandler(stream)\n","      \n","        logger.info('creating {}'.format(final_log_file))\n","    return logger\n","\n","\n","# define transforms\n","def get_transforms():\n","    # normlaize params for 3 channels\n","    mean=[0.485, 0.456, 0.406]\n","    std=[0.229, 0.224, 0.225]\n","\n","    # transform by timm\n","    train_transform = create_transform(\n","            input_size=img_size,\n","            is_training=True,\n","            color_jitter=0.4,\n","            auto_augment='rand-m9-mstd0.5-inc1',\n","            re_prob=0.25,\n","            re_mode='pixel',\n","            re_count=1,\n","            interpolation='bicubic')\n","    \n","    val_transform = transforms.Compose([\n","        transforms.Resize((img_size, img_size), \n","                          interpolation=InterpolationMode.BICUBIC), # interpolation 'bicubic'\n","        transforms.ToTensor(),\n","        transforms.Normalize(mean=mean, std=std)])\n","    return train_transform, val_transform\n","\n","\n","# load train, val images\n","def get_dataloaders(path, train_transform, val_transform):\n","    # imagenet dataset path\n","    train_path = os.path.join(path, 'train')\n","    val_path = os.path.join(path, 'val')\n","\n","    # import train and val dataset\n","    train_data = datasets.ImageFolder(root=train_path, transform=train_transform)\n","    val_data = datasets.ImageFolder(root=val_path, transform=val_transform)\n","    # train_data = torch.utils.data.Subset(train_data, np.random.choice(len(train_data), 10000, replace=False))\n","    # val_data = torch.utils.data.Subset(val_data, np.random.choice(len(train_data), 1000, replace=False))\n","    dataset_sizes = {'train': len(train_data), 'val': len(val_data)}\n","\n","    #load dataset into Dataloader\n","    train_loader = torch.utils.data.DataLoader(\n","        train_data, batch_size=batchsize, shuffle=True, \n","        pin_memory=True, num_workers=num_workers, drop_last=True)\n","\n","    val_loader = torch.utils.data.DataLoader(\n","        val_data, batch_size=200, shuffle=False, \n","        pin_memory=True, num_workers=num_workers, drop_last=False)\n","\n","    dataloaders = {'train': train_loader, 'val': val_loader}\n","    return dataloaders, dataset_sizes\n","\n","def get_grad_norm(parameters, norm_type=2):\n","    if isinstance(parameters, torch.Tensor):\n","        parameters = [parameters]\n","    parameters = list(filter(lambda p: p.grad is not None, parameters))\n","    norm_type = float(norm_type)\n","    total_norm = 0\n","    for p in parameters:\n","        param_norm = p.grad.data.norm(norm_type)\n","        total_norm += param_norm.item() ** norm_type\n","    total_norm = total_norm ** (1. / norm_type)\n","    return total_norm\n","\n","def train_one_epoch(model, epoch, dataloaders, optimizer, lr_scheduler, criterion, mixup_fn):\n","    model.train()\n","    logger.info('Train: ')\n","    \n","    running_loss = 0.0\n","    running_acc = 0.0\n","\n","    niter_per_epoch = len(dataloaders['train'])\n","    # iterate over dataset\n","    for idx, (images, labels) in enumerate(dataloaders['train']):\n","        images = images.to(device)\n","        labels = labels.to(device)\n","        \n","        # mixup labels during training\n","        images, labels = mixup_fn(images, labels)\n","\n","        optimizer.zero_grad()\n","\n","        outputs = model(images)\n","        _, preds = torch.max(outputs, 1)\n","        loss = criterion(outputs, labels)\n","\n","        loss.backward()\n","        # Clip gradient norm\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), 2.0)\n","        with torch.no_grad():\n","            grad_norm = get_grad_norm(model.parameters())\n","        optimizer.step()\n","        lr_scheduler.step_update(epoch*niter_per_epoch+idx) # timm scheduler\n","        \n","        running_loss += loss.item() * images.size(0)\n","        _, indices = torch.max(labels, 1)\n","        running_acc += torch.sum(preds == indices)\n","        \n","        if idx % print_freq == 0:\n","            lr = optimizer.param_groups[0]['lr']\n","            logger.info(\n","                f'Epoch [{epoch}/{num_epoches - 1}][{idx}/{niter_per_epoch}]\\t'\n","                f'lr: {lr:.7f}\\t'\n","                f'loss: {loss.item():.4f}\\t'\n","                f'grad norm: {grad_norm:.4f}'\n","                )\n","    print(dataset_sizes['train'])\n","    epoch_loss = running_loss / dataset_sizes['train']\n","    epoch_acc = running_acc.double() / dataset_sizes['train']\n","    logger.info(\n","        f'Train: epoch loss: {epoch_loss:.6f}\\t'\n","        f'epoch acc: {epoch_acc:.4f}')\n","    return epoch_loss, epoch_acc\n","\n","\n","@torch.no_grad()\n","def validate(model, epoch, dataloaders):\n","    model.eval()\n","    criterion = nn.CrossEntropyLoss()\n","    logger.info('Validate: ')         \n","    \n","    running_loss = 0.0\n","    running_acc = 0.0\n","    niter_per_epoch = len(dataloaders['val'])\n","    # iterate over dataset\n","    for idx, (images, labels) in enumerate(dataloaders['val']):\n","        images = images.to(device)\n","        labels = labels.to(device)\n","      \n","        outputs = model(images)\n","        _, preds = torch.max(outputs, 1)\n","        loss = criterion(outputs, labels)\n","\n","        running_loss += loss.item() * images.size(0)\n","        running_acc += torch.sum(preds == labels)\n","        \n","        if idx % print_freq == 0:\n","            logger.info(\n","                f'Epoch [{epoch}/{num_epoches - 1}][{idx}/{niter_per_epoch}]\\t'\n","                f'loss: {loss.item():.4f}')\n","    epoch_loss = running_loss / dataset_sizes['val']\n","    epoch_acc = running_acc.double() / dataset_sizes['val']\n","    logger.info(\n","        f'Val: epoch loss: {epoch_loss:.6f}\\t'\n","        f'epoch acc: {epoch_acc:.4f}')\n","    print(' ')\n","    print('=='*15)\n","    return epoch_loss, epoch_acc\n","\n","\n","def decay_filter(model):\n","    # set weight decay of normalization to 0.\n","    # len(param.shape) = 1 for all normalization layers' weights and bias\n","    # and all bias has length 1\n","    no_weight_decay = []\n","    has_weight_decay = []\n","    for name, param in model.named_parameters():\n","        # print(name, len(param.shape))\n","        if len(param.shape)== 1 or name.endswith('.relative_position_bias_table'):\n","            no_weight_decay.append(param)\n","        else:\n","            has_weight_decay.append(param)\n","    paramters = [{'params': no_weight_decay, \"weight_decay\": 0.0}, \n","                 {'params': has_weight_decay}]\n","    return paramters\n","\n","# define training process\n","def train(dataloaders, model, num_epoches=30, save_dir='./models/', model_name='swinT_'):\n","    # parameters = model.parameters()\n","    parameters = decay_filter(model)\n","    # lr=0.001, weight_decay=0.05\n","    optimizer = AdamW(parameters, eps=1e-8, betas=(0.9, 0.999), lr=0.0001, weight_decay=0.005)\n","    \n","    # scheduler from timm - cosine decay\n","    niter_per_epoch = len(dataloaders['train'])\n","    lr_scheduler = CosineLRScheduler(optimizer,\n","                                     t_initial=int(num_epoches*niter_per_epoch),\n","                                     lr_min=5e-6,\n","                                     warmup_lr_init=1e-7, # 5e-7 on github\n","                                     warmup_t=int(warmup_epoches*niter_per_epoch),\n","                                     t_in_epochs=False)\n","    '''\n","    lr_scheduler = StepLRScheduler(\n","            optimizer,\n","            decay_t=int(30*niter_per_epoch),\n","            decay_rate=0.1,\n","            warmup_lr_init=5e-7,\n","            warmup_t=int(warmup_epoches*niter_per_epoch),\n","            t_in_epochs=False)\n","    '''\n","    # loss function\n","    mixup_fn = Mixup(mixup_alpha=0.8, cutmix_alpha=1.0, cutmix_minmax=None,\n","                     prob=1.0, switch_prob=0.5, mode='batch',  # prob was 1.0 in github\n","                     label_smoothing=0.1, num_classes=num_classes) \n","    # criterion = nn.CrossEntropyLoss() # normal way\n","    criterion = SoftTargetCrossEntropy() # for mixup label transform\n","    \n","    # train and validate\n","    train_loss_history = []\n","    val_loss_history = []\n","    train_acc_history = []\n","    val_acc_history = []\n","\n","    best_tr_acc = 0.0\n","    best_val_acc = 0.0\n","\n","    logger.info(\"Start training-----------\")\n","    for epoch in range(num_epoches):\n","        # train and validate for one epoch\n","        epoch_loss, epoch_acc = train_one_epoch(model, epoch, dataloaders, \n","                                                optimizer, lr_scheduler, criterion, mixup_fn)\n","        \n","        if epoch_acc \u003e best_tr_acc:\n","            best_tr_acc = epoch_acc\n","        train_loss_history.append(epoch_loss)\n","        train_acc_history.append(epoch_acc.cpu())\n","\n","        epoch_loss, epoch_acc = validate(model, epoch, dataloaders)\n","        if epoch_acc \u003e best_val_acc:\n","            best_val_acc = epoch_acc\n","        val_loss_history.append(epoch_loss)\n","        val_acc_history.append(epoch_acc.cpu())\n","        \n","        # save model weights\n","        if save_dir:\n","            model_weights = copy.deepcopy(model.state_dict())\n","            torch.save(model_weights, os.path.join(save_dir, model_name + str(epoch) + '.pth'), \n","                        _use_new_zipfile_serialization=False)\n","                \n","    print('Best train Acc: {:4f}'.format(best_tr_acc))\n","    print('Best val Acc: {:4f}'.format(best_val_acc))\n","\n","    return train_loss_history, val_loss_history, train_acc_history, val_acc_history\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lJGUWOGyl1uT"},"outputs":[],"source":["# hyperparameters in paper\n","num_epoches = 100\n","warmup_epoches = 10\n","# num_epoches = 50\n","# warmup_epoches = 3\n","# batchsize = 1024\n","# batchsize = 512\n","batchsize = 128 # for debug\n","img_size = 224\n","patch_size = 4\n","embed_dim = 96\n","depths = [2, 2, 6, 2]\n","num_heads = [3, 6, 12, 24]\n","window_size = 7\n","drop_path_rate = 0.2\n","num_workers = 2 # in public code this is 8\n","print_freq = 10\n","\n","num_classes = 200 # for tiny-imagenet-200\n","# num_classes = 1000 # for imagenet-1k\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":13764,"status":"ok","timestamp":1650928027832,"user":{"displayName":"Jiangyan Feng","userId":"00140510051904189650"},"user_tz":240},"id":"ITFi2ZJP31Jb","outputId":"52e9a656-82c1-4087-c5d5-a974813e3158"},"outputs":[{"name":"stdout","output_type":"stream","text":["(56, 56)\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/torch/functional.py:568: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2228.)\n","  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"]},{"name":"stdout","output_type":"stream","text":["(28, 28)\n","(14, 14)\n","(14, 14)\n","(14, 14)\n","(7, 7)\n"]}],"source":["# init swin transformer model\n","model = SwinTransformerModel(\n","    img_size=img_size,\n","    patch_size=patch_size, \n","    in_chans=3, \n","    num_classes=num_classes,\n","    embed_dim=embed_dim, \n","    depths=depths, \n","    num_heads=num_heads,\n","    window_size=window_size,\n","    drop_path_rate=drop_path_rate\n","    )"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2723,"status":"ok","timestamp":1650928030534,"user":{"displayName":"Jiangyan Feng","userId":"00140510051904189650"},"user_tz":240},"id":"zWEbhASMTjiD","outputId":"ff825596-0b57-403e-a59d-87379fff39bb"},"outputs":[{"name":"stdout","output_type":"stream","text":["/content/drive/MyDrive/EECS542_project\n","SwinTransformerModel(\n","  (patch_embed): PatchEmbed(\n","    (linear): Conv2d(3, 96, kernel_size=(4, 4), stride=(4, 4))\n","    (norm): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n","  )\n","  (block1): SwinTransformerBlock(\n","    (LN1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n","    (LN2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n","    (LN3): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n","    (LN4): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n","    (MLP1): Mlp(\n","      (fc1): Linear(in_features=96, out_features=384, bias=True)\n","      (act): GELU()\n","      (fc2): Linear(in_features=384, out_features=96, bias=True)\n","      (drop): Dropout(p=0.0, inplace=False)\n","    )\n","    (MLP2): Mlp(\n","      (fc1): Linear(in_features=96, out_features=384, bias=True)\n","      (act): GELU()\n","      (fc2): Linear(in_features=384, out_features=96, bias=True)\n","      (drop): Dropout(p=0.0, inplace=False)\n","    )\n","    (WindowAttention1): WindowAttention(\n","      (qkv): Linear(in_features=96, out_features=288, bias=True)\n","      (attn_drop): Dropout(p=0.0, inplace=False)\n","      (proj): Linear(in_features=96, out_features=96, bias=True)\n","      (proj_drop): Dropout(p=0.0, inplace=False)\n","      (softmax): Softmax(dim=-1)\n","    )\n","    (WindowAttention2): WindowAttention(\n","      (qkv): Linear(in_features=96, out_features=288, bias=True)\n","      (attn_drop): Dropout(p=0.0, inplace=False)\n","      (proj): Linear(in_features=96, out_features=96, bias=True)\n","      (proj_drop): Dropout(p=0.0, inplace=False)\n","      (softmax): Softmax(dim=-1)\n","    )\n","    (drop_path1): DropPath()\n","    (drop_path2): DropPath()\n","  )\n","  (block2): SwinTransformerBlock(\n","    (LN1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n","    (LN2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n","    (LN3): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n","    (LN4): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n","    (MLP1): Mlp(\n","      (fc1): Linear(in_features=192, out_features=768, bias=True)\n","      (act): GELU()\n","      (fc2): Linear(in_features=768, out_features=192, bias=True)\n","      (drop): Dropout(p=0.0, inplace=False)\n","    )\n","    (MLP2): Mlp(\n","      (fc1): Linear(in_features=192, out_features=768, bias=True)\n","      (act): GELU()\n","      (fc2): Linear(in_features=768, out_features=192, bias=True)\n","      (drop): Dropout(p=0.0, inplace=False)\n","    )\n","    (WindowAttention1): WindowAttention(\n","      (qkv): Linear(in_features=192, out_features=576, bias=True)\n","      (attn_drop): Dropout(p=0.0, inplace=False)\n","      (proj): Linear(in_features=192, out_features=192, bias=True)\n","      (proj_drop): Dropout(p=0.0, inplace=False)\n","      (softmax): Softmax(dim=-1)\n","    )\n","    (WindowAttention2): WindowAttention(\n","      (qkv): Linear(in_features=192, out_features=576, bias=True)\n","      (attn_drop): Dropout(p=0.0, inplace=False)\n","      (proj): Linear(in_features=192, out_features=192, bias=True)\n","      (proj_drop): Dropout(p=0.0, inplace=False)\n","      (softmax): Softmax(dim=-1)\n","    )\n","    (drop_path1): DropPath()\n","    (drop_path2): DropPath()\n","  )\n","  (block3): ModuleList(\n","    (0): SwinTransformerBlock(\n","      (LN1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n","      (LN2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n","      (LN3): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n","      (LN4): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n","      (MLP1): Mlp(\n","        (fc1): Linear(in_features=384, out_features=1536, bias=True)\n","        (act): GELU()\n","        (fc2): Linear(in_features=1536, out_features=384, bias=True)\n","        (drop): Dropout(p=0.0, inplace=False)\n","      )\n","      (MLP2): Mlp(\n","        (fc1): Linear(in_features=384, out_features=1536, bias=True)\n","        (act): GELU()\n","        (fc2): Linear(in_features=1536, out_features=384, bias=True)\n","        (drop): Dropout(p=0.0, inplace=False)\n","      )\n","      (WindowAttention1): WindowAttention(\n","        (qkv): Linear(in_features=384, out_features=1152, bias=True)\n","        (attn_drop): Dropout(p=0.0, inplace=False)\n","        (proj): Linear(in_features=384, out_features=384, bias=True)\n","        (proj_drop): Dropout(p=0.0, inplace=False)\n","        (softmax): Softmax(dim=-1)\n","      )\n","      (WindowAttention2): WindowAttention(\n","        (qkv): Linear(in_features=384, out_features=1152, bias=True)\n","        (attn_drop): Dropout(p=0.0, inplace=False)\n","        (proj): Linear(in_features=384, out_features=384, bias=True)\n","        (proj_drop): Dropout(p=0.0, inplace=False)\n","        (softmax): Softmax(dim=-1)\n","      )\n","      (drop_path1): DropPath()\n","      (drop_path2): DropPath()\n","    )\n","    (1): SwinTransformerBlock(\n","      (LN1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n","      (LN2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n","      (LN3): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n","      (LN4): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n","      (MLP1): Mlp(\n","        (fc1): Linear(in_features=384, out_features=1536, bias=True)\n","        (act): GELU()\n","        (fc2): Linear(in_features=1536, out_features=384, bias=True)\n","        (drop): Dropout(p=0.0, inplace=False)\n","      )\n","      (MLP2): Mlp(\n","        (fc1): Linear(in_features=384, out_features=1536, bias=True)\n","        (act): GELU()\n","        (fc2): Linear(in_features=1536, out_features=384, bias=True)\n","        (drop): Dropout(p=0.0, inplace=False)\n","      )\n","      (WindowAttention1): WindowAttention(\n","        (qkv): Linear(in_features=384, out_features=1152, bias=True)\n","        (attn_drop): Dropout(p=0.0, inplace=False)\n","        (proj): Linear(in_features=384, out_features=384, bias=True)\n","        (proj_drop): Dropout(p=0.0, inplace=False)\n","        (softmax): Softmax(dim=-1)\n","      )\n","      (WindowAttention2): WindowAttention(\n","        (qkv): Linear(in_features=384, out_features=1152, bias=True)\n","        (attn_drop): Dropout(p=0.0, inplace=False)\n","        (proj): Linear(in_features=384, out_features=384, bias=True)\n","        (proj_drop): Dropout(p=0.0, inplace=False)\n","        (softmax): Softmax(dim=-1)\n","      )\n","      (drop_path1): DropPath()\n","      (drop_path2): DropPath()\n","    )\n","    (2): SwinTransformerBlock(\n","      (LN1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n","      (LN2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n","      (LN3): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n","      (LN4): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n","      (MLP1): Mlp(\n","        (fc1): Linear(in_features=384, out_features=1536, bias=True)\n","        (act): GELU()\n","        (fc2): Linear(in_features=1536, out_features=384, bias=True)\n","        (drop): Dropout(p=0.0, inplace=False)\n","      )\n","      (MLP2): Mlp(\n","        (fc1): Linear(in_features=384, out_features=1536, bias=True)\n","        (act): GELU()\n","        (fc2): Linear(in_features=1536, out_features=384, bias=True)\n","        (drop): Dropout(p=0.0, inplace=False)\n","      )\n","      (WindowAttention1): WindowAttention(\n","        (qkv): Linear(in_features=384, out_features=1152, bias=True)\n","        (attn_drop): Dropout(p=0.0, inplace=False)\n","        (proj): Linear(in_features=384, out_features=384, bias=True)\n","        (proj_drop): Dropout(p=0.0, inplace=False)\n","        (softmax): Softmax(dim=-1)\n","      )\n","      (WindowAttention2): WindowAttention(\n","        (qkv): Linear(in_features=384, out_features=1152, bias=True)\n","        (attn_drop): Dropout(p=0.0, inplace=False)\n","        (proj): Linear(in_features=384, out_features=384, bias=True)\n","        (proj_drop): Dropout(p=0.0, inplace=False)\n","        (softmax): Softmax(dim=-1)\n","      )\n","      (drop_path1): DropPath()\n","      (drop_path2): DropPath()\n","    )\n","  )\n","  (block4): SwinTransformerBlock(\n","    (LN1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","    (LN2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","    (LN3): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","    (LN4): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","    (MLP1): Mlp(\n","      (fc1): Linear(in_features=768, out_features=3072, bias=True)\n","      (act): GELU()\n","      (fc2): Linear(in_features=3072, out_features=768, bias=True)\n","      (drop): Dropout(p=0.0, inplace=False)\n","    )\n","    (MLP2): Mlp(\n","      (fc1): Linear(in_features=768, out_features=3072, bias=True)\n","      (act): GELU()\n","      (fc2): Linear(in_features=3072, out_features=768, bias=True)\n","      (drop): Dropout(p=0.0, inplace=False)\n","    )\n","    (WindowAttention1): WindowAttention(\n","      (qkv): Linear(in_features=768, out_features=2304, bias=True)\n","      (attn_drop): Dropout(p=0.0, inplace=False)\n","      (proj): Linear(in_features=768, out_features=768, bias=True)\n","      (proj_drop): Dropout(p=0.0, inplace=False)\n","      (softmax): Softmax(dim=-1)\n","    )\n","    (WindowAttention2): WindowAttention(\n","      (qkv): Linear(in_features=768, out_features=2304, bias=True)\n","      (attn_drop): Dropout(p=0.0, inplace=False)\n","      (proj): Linear(in_features=768, out_features=768, bias=True)\n","      (proj_drop): Dropout(p=0.0, inplace=False)\n","      (softmax): Softmax(dim=-1)\n","    )\n","    (drop_path1): DropPath()\n","    (drop_path2): DropPath()\n","  )\n","  (patch_merging1): PatchMerging(\n","    (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n","    (linear): Linear(in_features=384, out_features=192, bias=False)\n","  )\n","  (patch_merging2): PatchMerging(\n","    (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","    (linear): Linear(in_features=768, out_features=384, bias=False)\n","  )\n","  (patch_merging3): PatchMerging(\n","    (norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n","    (linear): Linear(in_features=1536, out_features=768, bias=False)\n","  )\n","  (final_layer): Linear(in_features=768, out_features=200, bias=True)\n","  (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","  (avgpool): AdaptiveAvgPool1d(output_size=1)\n","  (head): Linear(in_features=768, out_features=200, bias=True)\n",")\n"]}],"source":["# load pretrained \n","%cd '/content/drive/MyDrive/EECS542_project/'\n","\n","pretrain = './models/log021/swinT_210.pth'\n","model = model.to(device)\n","model.load_state_dict(torch.load(pretrain))\n","print(model)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":206118,"status":"ok","timestamp":1650928236650,"user":{"displayName":"Jiangyan Feng","userId":"00140510051904189650"},"user_tz":240},"id":"WP62M0mplVdh","outputId":"37cfbb63-e19f-4f29-89b7-1192d360c15a"},"outputs":[{"name":"stdout","output_type":"stream","text":["{'train': 100261, 'val': 10000}\n"]}],"source":["# transforms and dataloaders\n","# !cp -r /content/drive/MyDrive/EECS542_project/tiny-imagenet-200/ /content/data/\n","\n","imagenet_path = './tiny-imagenet-200/'\n","train_transform, val_transform = get_transforms()\n","dataloaders, dataset_sizes = get_dataloaders(imagenet_path, train_transform, val_transform)\n","print(dataset_sizes)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"SSutnsUkm3eZ"},"outputs":[{"name":"stderr","output_type":"stream","text":["[2022-04-26 08:23:54,193][line: 237] ==\u003e Start training-----------\n","[2022-04-26 08:23:54,198][line: 97] ==\u003e Train: \n","[2022-04-26 08:23:56,947][line: 132] ==\u003e Epoch [0/99][0/783]\tlr: 0.0000001\tloss: 3.1719\tgrad norm: 2.0000\n","[2022-04-26 08:24:04,444][line: 132] ==\u003e Epoch [0/99][10/783]\tlr: 0.0000002\tloss: 3.7946\tgrad norm: 2.0000\n","[2022-04-26 08:24:11,766][line: 132] ==\u003e Epoch [0/99][20/783]\tlr: 0.0000004\tloss: 2.2735\tgrad norm: 2.0000\n","[2022-04-26 08:24:19,220][line: 132] ==\u003e Epoch [0/99][30/783]\tlr: 0.0000005\tloss: 2.6768\tgrad norm: 2.0000\n","[2022-04-26 08:24:26,669][line: 132] ==\u003e Epoch [0/99][40/783]\tlr: 0.0000006\tloss: 3.8157\tgrad norm: 1.9178\n","[2022-04-26 08:24:33,654][line: 132] ==\u003e Epoch [0/99][50/783]\tlr: 0.0000007\tloss: 3.1721\tgrad norm: 2.0000\n","[2022-04-26 08:24:40,734][line: 132] ==\u003e Epoch [0/99][60/783]\tlr: 0.0000009\tloss: 3.5855\tgrad norm: 2.0000\n","[2022-04-26 08:24:47,979][line: 132] ==\u003e Epoch [0/99][70/783]\tlr: 0.0000010\tloss: 2.8225\tgrad norm: 2.0000\n","[2022-04-26 08:24:55,037][line: 132] ==\u003e Epoch [0/99][80/783]\tlr: 0.0000011\tloss: 3.8693\tgrad norm: 2.0000\n","[2022-04-26 08:25:02,290][line: 132] ==\u003e Epoch [0/99][90/783]\tlr: 0.0000012\tloss: 4.0276\tgrad norm: 1.9834\n","[2022-04-26 08:25:09,496][line: 132] ==\u003e Epoch [0/99][100/783]\tlr: 0.0000014\tloss: 3.4004\tgrad norm: 2.0000\n","[2022-04-26 08:25:16,598][line: 132] ==\u003e Epoch [0/99][110/783]\tlr: 0.0000015\tloss: 3.6827\tgrad norm: 2.0000\n","[2022-04-26 08:25:24,304][line: 132] ==\u003e Epoch [0/99][120/783]\tlr: 0.0000016\tloss: 3.1959\tgrad norm: 2.0000\n","[2022-04-26 08:25:32,204][line: 132] ==\u003e Epoch [0/99][130/783]\tlr: 0.0000018\tloss: 2.4724\tgrad norm: 2.0000\n","[2022-04-26 08:25:39,298][line: 132] ==\u003e Epoch [0/99][140/783]\tlr: 0.0000019\tloss: 4.1391\tgrad norm: 2.0000\n","[2022-04-26 08:25:46,309][line: 132] ==\u003e Epoch [0/99][150/783]\tlr: 0.0000020\tloss: 2.8907\tgrad norm: 2.0000\n","[2022-04-26 08:25:53,636][line: 132] ==\u003e Epoch [0/99][160/783]\tlr: 0.0000021\tloss: 3.5074\tgrad norm: 1.9404\n","[2022-04-26 08:26:00,836][line: 132] ==\u003e Epoch [0/99][170/783]\tlr: 0.0000023\tloss: 4.1691\tgrad norm: 2.0000\n","[2022-04-26 08:26:08,089][line: 132] ==\u003e Epoch [0/99][180/783]\tlr: 0.0000024\tloss: 3.4589\tgrad norm: 1.8013\n","[2022-04-26 08:26:15,467][line: 132] ==\u003e Epoch [0/99][190/783]\tlr: 0.0000025\tloss: 4.2871\tgrad norm: 2.0000\n","[2022-04-26 08:26:22,752][line: 132] ==\u003e Epoch [0/99][200/783]\tlr: 0.0000027\tloss: 3.7894\tgrad norm: 2.0000\n","[2022-04-26 08:26:29,852][line: 132] ==\u003e Epoch [0/99][210/783]\tlr: 0.0000028\tloss: 3.6167\tgrad norm: 2.0000\n","[2022-04-26 08:26:37,261][line: 132] ==\u003e Epoch [0/99][220/783]\tlr: 0.0000029\tloss: 4.2581\tgrad norm: 2.0000\n","[2022-04-26 08:26:44,527][line: 132] ==\u003e Epoch [0/99][230/783]\tlr: 0.0000030\tloss: 3.6353\tgrad norm: 2.0000\n","[2022-04-26 08:26:51,669][line: 132] ==\u003e Epoch [0/99][240/783]\tlr: 0.0000032\tloss: 2.4378\tgrad norm: 2.0000\n","[2022-04-26 08:26:58,630][line: 132] ==\u003e Epoch [0/99][250/783]\tlr: 0.0000033\tloss: 3.0013\tgrad norm: 1.8512\n","[2022-04-26 08:27:05,758][line: 132] ==\u003e Epoch [0/99][260/783]\tlr: 0.0000034\tloss: 3.3922\tgrad norm: 2.0000\n","[2022-04-26 08:27:13,063][line: 132] ==\u003e Epoch [0/99][270/783]\tlr: 0.0000035\tloss: 3.5417\tgrad norm: 2.0000\n","[2022-04-26 08:27:20,554][line: 132] ==\u003e Epoch [0/99][280/783]\tlr: 0.0000037\tloss: 3.6042\tgrad norm: 1.6118\n","[2022-04-26 08:27:27,616][line: 132] ==\u003e Epoch [0/99][290/783]\tlr: 0.0000038\tloss: 3.8439\tgrad norm: 2.0000\n","[2022-04-26 08:27:34,824][line: 132] ==\u003e Epoch [0/99][300/783]\tlr: 0.0000039\tloss: 3.4882\tgrad norm: 1.9660\n","[2022-04-26 08:27:42,122][line: 132] ==\u003e Epoch [0/99][310/783]\tlr: 0.0000041\tloss: 3.6225\tgrad norm: 2.0000\n","[2022-04-26 08:27:49,639][line: 132] ==\u003e Epoch [0/99][320/783]\tlr: 0.0000042\tloss: 4.0001\tgrad norm: 2.0000\n","[2022-04-26 08:27:57,095][line: 132] ==\u003e Epoch [0/99][330/783]\tlr: 0.0000043\tloss: 3.9060\tgrad norm: 1.9007\n","[2022-04-26 08:28:04,648][line: 132] ==\u003e Epoch [0/99][340/783]\tlr: 0.0000044\tloss: 3.8954\tgrad norm: 1.8006\n","[2022-04-26 08:28:12,067][line: 132] ==\u003e Epoch [0/99][350/783]\tlr: 0.0000046\tloss: 3.1726\tgrad norm: 2.0000\n","[2022-04-26 08:28:19,622][line: 132] ==\u003e Epoch [0/99][360/783]\tlr: 0.0000047\tloss: 3.3225\tgrad norm: 1.9773\n","[2022-04-26 08:28:26,755][line: 132] ==\u003e Epoch [0/99][370/783]\tlr: 0.0000048\tloss: 2.5278\tgrad norm: 2.0000\n","[2022-04-26 08:28:33,894][line: 132] ==\u003e Epoch [0/99][380/783]\tlr: 0.0000049\tloss: 3.7613\tgrad norm: 2.0000\n","[2022-04-26 08:28:41,228][line: 132] ==\u003e Epoch [0/99][390/783]\tlr: 0.0000051\tloss: 3.4362\tgrad norm: 2.0000\n","[2022-04-26 08:28:48,621][line: 132] ==\u003e Epoch [0/99][400/783]\tlr: 0.0000052\tloss: 3.6352\tgrad norm: 1.9500\n","[2022-04-26 08:28:56,098][line: 132] ==\u003e Epoch [0/99][410/783]\tlr: 0.0000053\tloss: 3.7836\tgrad norm: 1.9694\n","[2022-04-26 08:29:03,564][line: 132] ==\u003e Epoch [0/99][420/783]\tlr: 0.0000055\tloss: 3.5988\tgrad norm: 1.9183\n","[2022-04-26 08:29:10,913][line: 132] ==\u003e Epoch [0/99][430/783]\tlr: 0.0000056\tloss: 2.5947\tgrad norm: 2.0000\n","[2022-04-26 08:29:18,199][line: 132] ==\u003e Epoch [0/99][440/783]\tlr: 0.0000057\tloss: 3.2641\tgrad norm: 1.7975\n","[2022-04-26 08:29:25,959][line: 132] ==\u003e Epoch [0/99][450/783]\tlr: 0.0000058\tloss: 3.7899\tgrad norm: 1.7651\n","[2022-04-26 08:29:33,465][line: 132] ==\u003e Epoch [0/99][460/783]\tlr: 0.0000060\tloss: 3.5790\tgrad norm: 1.7731\n","[2022-04-26 08:29:40,968][line: 132] ==\u003e Epoch [0/99][470/783]\tlr: 0.0000061\tloss: 3.6928\tgrad norm: 1.8541\n","[2022-04-26 08:29:48,554][line: 132] ==\u003e Epoch [0/99][480/783]\tlr: 0.0000062\tloss: 2.4271\tgrad norm: 2.0000\n","[2022-04-26 08:29:56,517][line: 132] ==\u003e Epoch [0/99][490/783]\tlr: 0.0000064\tloss: 3.8223\tgrad norm: 1.9225\n","[2022-04-26 08:30:04,223][line: 132] ==\u003e Epoch [0/99][500/783]\tlr: 0.0000065\tloss: 3.5914\tgrad norm: 1.8770\n","[2022-04-26 08:30:12,126][line: 132] ==\u003e Epoch [0/99][510/783]\tlr: 0.0000066\tloss: 3.4967\tgrad norm: 1.8299\n","[2022-04-26 08:30:19,711][line: 132] ==\u003e Epoch [0/99][520/783]\tlr: 0.0000067\tloss: 2.9577\tgrad norm: 2.0000\n","[2022-04-26 08:30:27,696][line: 132] ==\u003e Epoch [0/99][530/783]\tlr: 0.0000069\tloss: 3.4454\tgrad norm: 1.5421\n","[2022-04-26 08:30:35,549][line: 132] ==\u003e Epoch [0/99][540/783]\tlr: 0.0000070\tloss: 2.5864\tgrad norm: 2.0000\n","[2022-04-26 08:30:43,041][line: 132] ==\u003e Epoch [0/99][550/783]\tlr: 0.0000071\tloss: 3.6103\tgrad norm: 1.8891\n","[2022-04-26 08:30:50,369][line: 132] ==\u003e Epoch [0/99][560/783]\tlr: 0.0000072\tloss: 3.7193\tgrad norm: 1.9758\n","[2022-04-26 08:30:57,822][line: 132] ==\u003e Epoch [0/99][570/783]\tlr: 0.0000074\tloss: 2.6758\tgrad norm: 2.0000\n","[2022-04-26 08:31:05,160][line: 132] ==\u003e Epoch [0/99][580/783]\tlr: 0.0000075\tloss: 3.2964\tgrad norm: 2.0000\n","[2022-04-26 08:31:12,495][line: 132] ==\u003e Epoch [0/99][590/783]\tlr: 0.0000076\tloss: 3.6260\tgrad norm: 1.9027\n","[2022-04-26 08:31:20,054][line: 132] ==\u003e Epoch [0/99][600/783]\tlr: 0.0000078\tloss: 2.5773\tgrad norm: 1.8889\n","[2022-04-26 08:31:27,294][line: 132] ==\u003e Epoch [0/99][610/783]\tlr: 0.0000079\tloss: 3.6560\tgrad norm: 1.7964\n","[2022-04-26 08:31:34,608][line: 132] ==\u003e Epoch [0/99][620/783]\tlr: 0.0000080\tloss: 3.8407\tgrad norm: 1.8234\n","[2022-04-26 08:31:42,021][line: 132] ==\u003e Epoch [0/99][630/783]\tlr: 0.0000081\tloss: 3.4412\tgrad norm: 1.8302\n","[2022-04-26 08:31:49,225][line: 132] ==\u003e Epoch [0/99][640/783]\tlr: 0.0000083\tloss: 4.1577\tgrad norm: 1.7941\n","[2022-04-26 08:31:56,656][line: 132] ==\u003e Epoch [0/99][650/783]\tlr: 0.0000084\tloss: 3.2994\tgrad norm: 2.0000\n","[2022-04-26 08:32:03,782][line: 132] ==\u003e Epoch [0/99][660/783]\tlr: 0.0000085\tloss: 3.6263\tgrad norm: 1.5730\n","[2022-04-26 08:32:11,186][line: 132] ==\u003e Epoch [0/99][670/783]\tlr: 0.0000086\tloss: 3.9303\tgrad norm: 1.9311\n","[2022-04-26 08:32:18,428][line: 132] ==\u003e Epoch [0/99][680/783]\tlr: 0.0000088\tloss: 3.8790\tgrad norm: 2.0000\n","[2022-04-26 08:32:25,933][line: 132] ==\u003e Epoch [0/99][690/783]\tlr: 0.0000089\tloss: 3.7246\tgrad norm: 1.9603\n","[2022-04-26 08:32:33,099][line: 132] ==\u003e Epoch [0/99][700/783]\tlr: 0.0000090\tloss: 4.0792\tgrad norm: 2.0000\n","[2022-04-26 08:32:40,770][line: 132] ==\u003e Epoch [0/99][710/783]\tlr: 0.0000092\tloss: 3.3106\tgrad norm: 2.0000\n","[2022-04-26 08:32:48,711][line: 132] ==\u003e Epoch [0/99][720/783]\tlr: 0.0000093\tloss: 3.4726\tgrad norm: 1.6012\n","[2022-04-26 08:32:56,676][line: 132] ==\u003e Epoch [0/99][730/783]\tlr: 0.0000094\tloss: 4.1354\tgrad norm: 2.0000\n","[2022-04-26 08:33:04,416][line: 132] ==\u003e Epoch [0/99][740/783]\tlr: 0.0000095\tloss: 3.7630\tgrad norm: 1.9005\n","[2022-04-26 08:33:12,410][line: 132] ==\u003e Epoch [0/99][750/783]\tlr: 0.0000097\tloss: 3.0206\tgrad norm: 2.0000\n","[2022-04-26 08:33:20,243][line: 132] ==\u003e Epoch [0/99][760/783]\tlr: 0.0000098\tloss: 3.7801\tgrad norm: 1.7354\n","[2022-04-26 08:33:28,082][line: 132] ==\u003e Epoch [0/99][770/783]\tlr: 0.0000099\tloss: 3.1658\tgrad norm: 2.0000\n","[2022-04-26 08:33:35,440][line: 132] ==\u003e Epoch [0/99][780/783]\tlr: 0.0000101\tloss: 3.3300\tgrad norm: 1.9330\n","[2022-04-26 08:33:36,598][line: 141] ==\u003e Train: epoch loss: 3.403205\tepoch acc: 0.4631\n","[2022-04-26 08:33:36,603][line: 150] ==\u003e Validate: \n"]},{"name":"stdout","output_type":"stream","text":["100261\n"]},{"name":"stderr","output_type":"stream","text":["[2022-04-26 08:33:38,368][line: 169] ==\u003e Epoch [0/99][0/50]\tloss: 1.0043\n","[2022-04-26 08:33:45,419][line: 169] ==\u003e Epoch [0/99][10/50]\tloss: 1.7498\n","[2022-04-26 08:33:52,331][line: 169] ==\u003e Epoch [0/99][20/50]\tloss: 1.6558\n","[2022-04-26 08:33:59,219][line: 169] ==\u003e Epoch [0/99][30/50]\tloss: 1.9630\n","[2022-04-26 08:34:06,189][line: 169] ==\u003e Epoch [0/99][40/50]\tloss: 1.5559\n","[2022-04-26 08:34:11,835][line: 174] ==\u003e Val: epoch loss: 1.505751\tepoch acc: 0.6613\n"]},{"name":"stdout","output_type":"stream","text":[" \n","==============================\n"]},{"name":"stderr","output_type":"stream","text":["[2022-04-26 08:34:12,195][line: 97] ==\u003e Train: \n","[2022-04-26 08:34:14,264][line: 132] ==\u003e Epoch [1/99][0/783]\tlr: 0.0000101\tloss: 2.4126\tgrad norm: 2.0000\n","[2022-04-26 08:34:22,333][line: 132] ==\u003e Epoch [1/99][10/783]\tlr: 0.0000102\tloss: 3.7527\tgrad norm: 1.5011\n","[2022-04-26 08:34:30,044][line: 132] ==\u003e Epoch [1/99][20/783]\tlr: 0.0000103\tloss: 4.0022\tgrad norm: 2.0000\n","[2022-04-26 08:34:37,827][line: 132] ==\u003e Epoch [1/99][30/783]\tlr: 0.0000105\tloss: 3.6228\tgrad norm: 1.5142\n","[2022-04-26 08:34:45,689][line: 132] ==\u003e Epoch [1/99][40/783]\tlr: 0.0000106\tloss: 4.1558\tgrad norm: 2.0000\n","[2022-04-26 08:34:53,678][line: 132] ==\u003e Epoch [1/99][50/783]\tlr: 0.0000107\tloss: 3.8928\tgrad norm: 1.7571\n","[2022-04-26 08:35:01,656][line: 132] ==\u003e Epoch [1/99][60/783]\tlr: 0.0000109\tloss: 3.8377\tgrad norm: 1.6968\n","[2022-04-26 08:35:09,577][line: 132] ==\u003e Epoch [1/99][70/783]\tlr: 0.0000110\tloss: 4.2779\tgrad norm: 1.6926\n","[2022-04-26 08:35:17,292][line: 132] ==\u003e Epoch [1/99][80/783]\tlr: 0.0000111\tloss: 2.8522\tgrad norm: 1.9752\n","[2022-04-26 08:35:25,573][line: 132] ==\u003e Epoch [1/99][90/783]\tlr: 0.0000112\tloss: 3.5978\tgrad norm: 1.7903\n","[2022-04-26 08:35:33,771][line: 132] ==\u003e Epoch [1/99][100/783]\tlr: 0.0000114\tloss: 2.5320\tgrad norm: 2.0000\n","[2022-04-26 08:35:41,534][line: 132] ==\u003e Epoch [1/99][110/783]\tlr: 0.0000115\tloss: 2.0816\tgrad norm: 2.0000\n","[2022-04-26 08:35:49,643][line: 132] ==\u003e Epoch [1/99][120/783]\tlr: 0.0000116\tloss: 2.0323\tgrad norm: 2.0000\n","[2022-04-26 08:35:57,546][line: 132] ==\u003e Epoch [1/99][130/783]\tlr: 0.0000117\tloss: 2.7896\tgrad norm: 1.7536\n","[2022-04-26 08:36:05,297][line: 132] ==\u003e Epoch [1/99][140/783]\tlr: 0.0000119\tloss: 3.2479\tgrad norm: 1.9376\n","[2022-04-26 08:36:13,147][line: 132] ==\u003e Epoch [1/99][150/783]\tlr: 0.0000120\tloss: 2.9134\tgrad norm: 2.0000\n","[2022-04-26 08:36:20,681][line: 132] ==\u003e Epoch [1/99][160/783]\tlr: 0.0000121\tloss: 3.7640\tgrad norm: 2.0000\n","[2022-04-26 08:36:28,598][line: 132] ==\u003e Epoch [1/99][170/783]\tlr: 0.0000123\tloss: 3.6839\tgrad norm: 1.8780\n","[2022-04-26 08:36:36,587][line: 132] ==\u003e Epoch [1/99][180/783]\tlr: 0.0000124\tloss: 4.2395\tgrad norm: 1.9741\n","[2022-04-26 08:36:44,649][line: 132] ==\u003e Epoch [1/99][190/783]\tlr: 0.0000125\tloss: 2.6857\tgrad norm: 2.0000\n","[2022-04-26 08:36:52,439][line: 132] ==\u003e Epoch [1/99][200/783]\tlr: 0.0000126\tloss: 3.3139\tgrad norm: 2.0000\n","[2022-04-26 08:37:00,417][line: 132] ==\u003e Epoch [1/99][210/783]\tlr: 0.0000128\tloss: 3.1883\tgrad norm: 1.7168\n","[2022-04-26 08:37:08,481][line: 132] ==\u003e Epoch [1/99][220/783]\tlr: 0.0000129\tloss: 2.4876\tgrad norm: 2.0000\n","[2022-04-26 08:37:16,187][line: 132] ==\u003e Epoch [1/99][230/783]\tlr: 0.0000130\tloss: 3.5842\tgrad norm: 1.7647\n","[2022-04-26 08:37:24,229][line: 132] ==\u003e Epoch [1/99][240/783]\tlr: 0.0000132\tloss: 3.5383\tgrad norm: 1.7591\n","[2022-04-26 08:37:32,118][line: 132] ==\u003e Epoch [1/99][250/783]\tlr: 0.0000133\tloss: 3.6853\tgrad norm: 1.6471\n","[2022-04-26 08:37:40,270][line: 132] ==\u003e Epoch [1/99][260/783]\tlr: 0.0000134\tloss: 3.2830\tgrad norm: 2.0000\n","[2022-04-26 08:37:48,314][line: 132] ==\u003e Epoch [1/99][270/783]\tlr: 0.0000135\tloss: 3.6880\tgrad norm: 2.0000\n","[2022-04-26 08:37:56,036][line: 132] ==\u003e Epoch [1/99][280/783]\tlr: 0.0000137\tloss: 3.1947\tgrad norm: 2.0000\n","[2022-04-26 08:38:03,935][line: 132] ==\u003e Epoch [1/99][290/783]\tlr: 0.0000138\tloss: 3.9645\tgrad norm: 1.7313\n","[2022-04-26 08:38:12,075][line: 132] ==\u003e Epoch [1/99][300/783]\tlr: 0.0000139\tloss: 3.4319\tgrad norm: 2.0000\n","[2022-04-26 08:38:20,134][line: 132] ==\u003e Epoch [1/99][310/783]\tlr: 0.0000140\tloss: 3.8400\tgrad norm: 2.0000\n","[2022-04-26 08:38:27,864][line: 132] ==\u003e Epoch [1/99][320/783]\tlr: 0.0000142\tloss: 3.2417\tgrad norm: 1.7373\n","[2022-04-26 08:38:35,806][line: 132] ==\u003e Epoch [1/99][330/783]\tlr: 0.0000143\tloss: 3.3342\tgrad norm: 1.7979\n","[2022-04-26 08:38:43,922][line: 132] ==\u003e Epoch [1/99][340/783]\tlr: 0.0000144\tloss: 4.0183\tgrad norm: 1.6882\n","[2022-04-26 08:38:51,927][line: 132] ==\u003e Epoch [1/99][350/783]\tlr: 0.0000146\tloss: 2.9301\tgrad norm: 2.0000\n","[2022-04-26 08:38:59,720][line: 132] ==\u003e Epoch [1/99][360/783]\tlr: 0.0000147\tloss: 4.1019\tgrad norm: 2.0000\n","[2022-04-26 08:39:07,176][line: 132] ==\u003e Epoch [1/99][370/783]\tlr: 0.0000148\tloss: 3.5038\tgrad norm: 1.7192\n","[2022-04-26 08:39:15,412][line: 132] ==\u003e Epoch [1/99][380/783]\tlr: 0.0000149\tloss: 4.1899\tgrad norm: 1.6340\n","[2022-04-26 08:39:23,647][line: 132] ==\u003e Epoch [1/99][390/783]\tlr: 0.0000151\tloss: 2.5455\tgrad norm: 2.0000\n","[2022-04-26 08:39:31,600][line: 132] ==\u003e Epoch [1/99][400/783]\tlr: 0.0000152\tloss: 3.7077\tgrad norm: 2.0000\n","[2022-04-26 08:39:39,636][line: 132] ==\u003e Epoch [1/99][410/783]\tlr: 0.0000153\tloss: 3.7193\tgrad norm: 2.0000\n","[2022-04-26 08:39:47,409][line: 132] ==\u003e Epoch [1/99][420/783]\tlr: 0.0000154\tloss: 2.4018\tgrad norm: 2.0000\n","[2022-04-26 08:39:55,151][line: 132] ==\u003e Epoch [1/99][430/783]\tlr: 0.0000156\tloss: 4.4148\tgrad norm: 1.8439\n","[2022-04-26 08:40:03,190][line: 132] ==\u003e Epoch [1/99][440/783]\tlr: 0.0000157\tloss: 2.7516\tgrad norm: 2.0000\n","[2022-04-26 08:40:10,782][line: 132] ==\u003e Epoch [1/99][450/783]\tlr: 0.0000158\tloss: 2.8678\tgrad norm: 1.7850\n","[2022-04-26 08:40:18,810][line: 132] ==\u003e Epoch [1/99][460/783]\tlr: 0.0000160\tloss: 4.0219\tgrad norm: 2.0000\n","[2022-04-26 08:40:26,867][line: 132] ==\u003e Epoch [1/99][470/783]\tlr: 0.0000161\tloss: 3.9849\tgrad norm: 1.9837\n","[2022-04-26 08:40:34,884][line: 132] ==\u003e Epoch [1/99][480/783]\tlr: 0.0000162\tloss: 3.7996\tgrad norm: 1.7325\n","[2022-04-26 08:40:42,873][line: 132] ==\u003e Epoch [1/99][490/783]\tlr: 0.0000163\tloss: 3.8369\tgrad norm: 1.8269\n","[2022-04-26 08:40:50,558][line: 132] ==\u003e Epoch [1/99][500/783]\tlr: 0.0000165\tloss: 2.5097\tgrad norm: 2.0000\n","[2022-04-26 08:40:58,734][line: 132] ==\u003e Epoch [1/99][510/783]\tlr: 0.0000166\tloss: 3.0747\tgrad norm: 2.0000\n","[2022-04-26 08:41:06,609][line: 132] ==\u003e Epoch [1/99][520/783]\tlr: 0.0000167\tloss: 4.2007\tgrad norm: 2.0000\n","[2022-04-26 08:41:14,392][line: 132] ==\u003e Epoch [1/99][530/783]\tlr: 0.0000169\tloss: 3.7878\tgrad norm: 2.0000\n","[2022-04-26 08:41:22,123][line: 132] ==\u003e Epoch [1/99][540/783]\tlr: 0.0000170\tloss: 3.4483\tgrad norm: 2.0000\n","[2022-04-26 08:41:30,165][line: 132] ==\u003e Epoch [1/99][550/783]\tlr: 0.0000171\tloss: 3.1101\tgrad norm: 2.0000\n","[2022-04-26 08:41:38,065][line: 132] ==\u003e Epoch [1/99][560/783]\tlr: 0.0000172\tloss: 3.8133\tgrad norm: 1.9562\n","[2022-04-26 08:41:45,909][line: 132] ==\u003e Epoch [1/99][570/783]\tlr: 0.0000174\tloss: 3.4444\tgrad norm: 1.9180\n","[2022-04-26 08:41:53,841][line: 132] ==\u003e Epoch [1/99][580/783]\tlr: 0.0000175\tloss: 3.2700\tgrad norm: 1.7819\n","[2022-04-26 08:42:01,572][line: 132] ==\u003e Epoch [1/99][590/783]\tlr: 0.0000176\tloss: 3.7134\tgrad norm: 2.0000\n","[2022-04-26 08:42:09,696][line: 132] ==\u003e Epoch [1/99][600/783]\tlr: 0.0000177\tloss: 2.4770\tgrad norm: 2.0000\n","[2022-04-26 08:42:17,433][line: 132] ==\u003e Epoch [1/99][610/783]\tlr: 0.0000179\tloss: 3.9289\tgrad norm: 1.6299\n","[2022-04-26 08:42:25,282][line: 132] ==\u003e Epoch [1/99][620/783]\tlr: 0.0000180\tloss: 4.1001\tgrad norm: 1.7853\n","[2022-04-26 08:42:33,229][line: 132] ==\u003e Epoch [1/99][630/783]\tlr: 0.0000181\tloss: 4.2988\tgrad norm: 1.8798\n","[2022-04-26 08:42:41,177][line: 132] ==\u003e Epoch [1/99][640/783]\tlr: 0.0000183\tloss: 3.6608\tgrad norm: 1.8472\n","[2022-04-26 08:42:48,985][line: 132] ==\u003e Epoch [1/99][650/783]\tlr: 0.0000184\tloss: 3.6636\tgrad norm: 2.0000\n","[2022-04-26 08:42:56,928][line: 132] ==\u003e Epoch [1/99][660/783]\tlr: 0.0000185\tloss: 3.4806\tgrad norm: 1.7213\n","[2022-04-26 08:43:04,709][line: 132] ==\u003e Epoch [1/99][670/783]\tlr: 0.0000186\tloss: 3.5201\tgrad norm: 1.9715\n","[2022-04-26 08:43:12,403][line: 132] ==\u003e Epoch [1/99][680/783]\tlr: 0.0000188\tloss: 2.9693\tgrad norm: 2.0000\n","[2022-04-26 08:43:20,242][line: 132] ==\u003e Epoch [1/99][690/783]\tlr: 0.0000189\tloss: 3.8049\tgrad norm: 1.8479\n","[2022-04-26 08:43:27,908][line: 132] ==\u003e Epoch [1/99][700/783]\tlr: 0.0000190\tloss: 2.8044\tgrad norm: 2.0000\n","[2022-04-26 08:43:35,598][line: 132] ==\u003e Epoch [1/99][710/783]\tlr: 0.0000191\tloss: 3.3409\tgrad norm: 1.8362\n","[2022-04-26 08:43:43,424][line: 132] ==\u003e Epoch [1/99][720/783]\tlr: 0.0000193\tloss: 2.6017\tgrad norm: 2.0000\n","[2022-04-26 08:43:51,511][line: 132] ==\u003e Epoch [1/99][730/783]\tlr: 0.0000194\tloss: 3.0294\tgrad norm: 2.0000\n","[2022-04-26 08:43:59,341][line: 132] ==\u003e Epoch [1/99][740/783]\tlr: 0.0000195\tloss: 3.2406\tgrad norm: 1.9661\n","[2022-04-26 08:44:07,233][line: 132] ==\u003e Epoch [1/99][750/783]\tlr: 0.0000197\tloss: 3.0889\tgrad norm: 2.0000\n","[2022-04-26 08:44:15,214][line: 132] ==\u003e Epoch [1/99][760/783]\tlr: 0.0000198\tloss: 2.6797\tgrad norm: 2.0000\n","[2022-04-26 08:44:22,869][line: 132] ==\u003e Epoch [1/99][770/783]\tlr: 0.0000199\tloss: 3.4742\tgrad norm: 2.0000\n","[2022-04-26 08:44:30,815][line: 132] ==\u003e Epoch [1/99][780/783]\tlr: 0.0000200\tloss: 3.8131\tgrad norm: 1.9417\n","[2022-04-26 08:44:32,317][line: 141] ==\u003e Train: epoch loss: 3.350745\tepoch acc: 0.4730\n","[2022-04-26 08:44:32,322][line: 150] ==\u003e Validate: \n"]},{"name":"stdout","output_type":"stream","text":["100261\n"]},{"name":"stderr","output_type":"stream","text":["[2022-04-26 08:44:33,980][line: 169] ==\u003e Epoch [1/99][0/50]\tloss: 1.0103\n","[2022-04-26 08:44:40,866][line: 169] ==\u003e Epoch [1/99][10/50]\tloss: 1.6679\n","[2022-04-26 08:44:47,741][line: 169] ==\u003e Epoch [1/99][20/50]\tloss: 1.6118\n","[2022-04-26 08:44:54,625][line: 169] ==\u003e Epoch [1/99][30/50]\tloss: 1.8387\n","[2022-04-26 08:45:01,098][line: 169] ==\u003e Epoch [1/99][40/50]\tloss: 1.5442\n","[2022-04-26 08:45:06,822][line: 174] ==\u003e Val: epoch loss: 1.476737\tepoch acc: 0.6677\n"]},{"name":"stdout","output_type":"stream","text":[" \n","==============================\n"]},{"name":"stderr","output_type":"stream","text":["[2022-04-26 08:45:07,188][line: 97] ==\u003e Train: \n"]},{"ename":"RuntimeError","evalue":"ignored","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m\u003cipython-input-14-bbd2f1538599\u003e\u001b[0m in \u001b[0;36m\u003cmodule\u003e\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mlogger\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlog_creater\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msave_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----\u003e 5\u001b[0;31m \u001b[0mtrain_loss_history\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loss_history\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_acc_history\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_acc_history\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epoches\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_epoches\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msave_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'swinT_'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m\u003cipython-input-13-e1988c22dcce\u003e\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(dataloaders, model, num_epoches, save_dir, model_name)\u001b[0m\n\u001b[1;32m    239\u001b[0m         \u001b[0;31m# train and validate for one epoch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m         epoch_loss, epoch_acc = train_one_epoch(model, epoch, dataloaders, \n\u001b[0;32m--\u003e 241\u001b[0;31m                                                 optimizer, lr_scheduler, criterion, mixup_fn)\n\u001b[0m\u001b[1;32m    242\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mepoch_acc\u001b[0m \u001b[0;34m\u003e\u001b[0m \u001b[0mbest_tr_acc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m\u003cipython-input-13-e1988c22dcce\u003e\u001b[0m in \u001b[0;36mtrain_one_epoch\u001b[0;34m(model, epoch, dataloaders, optimizer, lr_scheduler, criterion, mixup_fn)\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--\u003e 117\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m         \u001b[0;31m# Clip gradient norm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    361\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m                 inputs=inputs)\n\u001b[0;32m--\u003e 363\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    173\u001b[0m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[1;32m    174\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--\u003e 175\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m def grad(\n","\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 74.00 MiB (GPU 0; 15.78 GiB total capacity; 14.06 GiB already allocated; 42.75 MiB free; 14.32 GiB reserved in total by PyTorch) If reserved memory is \u003e\u003e allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"]}],"source":["# training\n","save_dir = './models/log022/'\n","logger = log_creater(save_dir)\n","\n","train_loss_history, val_loss_history, train_acc_history, val_acc_history = \\\n","train(dataloaders, model, num_epoches=num_epoches, save_dir=save_dir, model_name='swinT_')\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dzvdEOYHnA-Z"},"outputs":[],"source":["import matplotlib.pyplot as plt\n","# draw figures\n","print(train_loss_history)\n","print(val_loss_history)\n","print(train_acc_history)\n","print(val_loss_history)\n","\n","x = list(range(num_epoches))\n","plt.figure()\n","plt.plot(x, train_loss_history)\n","plt.plot(x, val_loss_history)\n","plt.title('loss history')\n","plt.show()\n","\n","plt.figure()\n","plt.plot(x, train_acc_history)\n","plt.plot(x, val_acc_history)\n","plt.title('acc history')\n","plt.show()\n"]}],"metadata":{"accelerator":"GPU","colab":{"background_execution":"on","collapsed_sections":[],"name":"Swin_Transformer_copy.ipynb","version":""},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}